{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "import tiktoken\n",
    "import langchain\n",
    "import pydantic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 0.2.16\n",
      "Pydantic version: 2.9.2\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "import pydantic\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n",
    "print(f\"Pydantic version: {pydantic.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ConfigDict\n",
    "from langchain_text_splitters.base import TextSplitter\n",
    "\n",
    "# Create a new configuration\n",
    "config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "# Apply this configuration to BaseModel\n",
    "BaseModel.model_config = config\n",
    "\n",
    "# If TextSplitter is not already a subclass of BaseModel, make it one\n",
    "if not issubclass(TextSplitter, BaseModel):\n",
    "    class PydanticTextSplitter(TextSplitter, BaseModel):\n",
    "        pass\n",
    "    TextSplitter = PydanticTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters.base import TextSplitter\n",
    "from pydantic_core import CoreSchema, core_schema\n",
    "\n",
    "def get_pydantic_core_schema(*args, **kwargs) -> CoreSchema:\n",
    "    return core_schema.any_schema()\n",
    "\n",
    "TextSplitter.__get_pydantic_core_schema__ = classmethod(get_pydantic_core_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import pinecone\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loader = PyPDFDirectoryLoader(\"pdf\") # folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 0}, page_content='Citation: Hussain, M. YOLO-v1 to\\nYOLO-v8, the Rise of YOLO and Its\\nComplementary Nature toward\\nDigital Manufacturing and Industrial\\nDefect Detection. Machines 2023 ,11,\\n677. https://doi.org/10.3390/\\nmachines11070677\\nAcademic Editor: Sang Do Noh\\nReceived: 30 May 2023\\nRevised: 15 June 2023\\nAccepted: 21 June 2023\\nPublished: 23 June 2023\\nCopyright: © 2023 by the author.\\nLicensee MDPI, Basel, Switzerland.\\nThis article is an open access article\\ndistributed under the terms and\\nconditions of the Creative Commons\\nAttribution (CC BY) license (https://\\ncreativecommons.org/licenses/by/\\n4.0/).\\nmachines\\nReview\\nYOLO-v1 to YOLO-v8, the Rise of YOLO and Its\\nComplementary Nature toward Digital Manufacturing and\\nIndustrial Defect Detection\\nMuhammad Hussain\\nDepartment of Computer Science, School of Computing and Engineering, University of Huddersﬁeld,\\nQueensgate, Huddersﬁeld HD1 3DH, UK; m.hussain@hud.ac.uk\\nAbstract: Since its inception in 2015, the YOLO (You Only Look Once) variant of object detectors has\\nrapidly grown, with the latest release of YOLO-v8 in January 2023. YOLO variants are underpinned\\nby the principle of real-time and high-classiﬁcation performance, based on limited but efﬁcient\\ncomputational parameters. This principle has been found within the DNA of all YOLO variants\\nwith increasing intensity, as the variants evolve addressing the requirements of automated quality\\ninspection within the industrial surface defect detection domain, such as the need for fast detection,\\nhigh accuracy, and deployment onto constrained edge devices. This paper is the ﬁrst to provide an\\nin-depth review of the YOLO evolution from the original YOLO to the recent release (YOLO-v8) from\\nthe perspective of industrial manufacturing. The review explores the key architectural advancements\\nproposed at each iteration, followed by examples of industrial deployment for surface defect detection\\nendorsing its compatibility with industrial requirements.\\nKeywords: industrial defect detection; object detection; smart manufacturing; quality inspection\\n1. Introduction\\nHumans via the visual cortex, a primary cortical region of the brain responsible for\\nprocessing visual information [ 1], are able to observe, recognize [ 2], and differentiate\\nbetween objects instantaneously [ 3]. Studying the inner workings of the visual cortex and\\nthe brain in general has paved the way for artiﬁcial neural networks (ANNs) [ 4] along\\nwith a myriad of computational architectures residing under the deep learning umbrella.\\nIn the last decade, owing to rapid and revolutionary advancements in the ﬁeld of deep\\nlearning [5], researchers have exerted their efforts on providing efﬁcient simulation of the\\nhuman visual system to computers, i.e., enabling computers to detect objects of interest\\nwithin static images and video [ 6], a ﬁeld known as computer vision (CV) [ 7]. CV is\\na prevalent research area for deep learning researchers and practitioners in the present\\ndecade. It is composed of subﬁelds consisting of image classiﬁcation [ 8], object detection [ 9],\\nand object segmentation [ 10]. All three ﬁelds share a common architectural theme, namely,\\nmanipulation of convolutional neural networks (CNNs) [ 11]. CNNs are accepted as the de\\nfacto when dealing with image data. In comparison with conventional image processing\\nand artiﬁcial defection methods, CNNs utilize multiple convolutional layers coupled with\\naggregation, i.e., pooling structures aiming to unearth deep semantic features hidden away\\nwithin the pixels of the image [12].\\nArtiﬁcial intelligence (AI) has found opportunities in industries across the spectrum\\nfrom renewable energy [ 13,14] and security to healthcare [ 15] and the education sector.\\nHowever, one industry that is poised for signiﬁcant automation through CV is the manu-\\nfacturing industry. Quality inspection (QI) is an integral part of any manufacturing domain\\nproviding integrity and conﬁdence to the clients on the quality of the manufactured prod-\\nucts [ 16]. Manufacturing has wide scope for automation; however, when dealing with\\nsurface inspection [ 17], defects can take sophisticated forms [ 18], making human-based\\nMachines 2023 ,11, 677. https://doi.org/10.3390/machines11070677 https://www.mdpi.com/journal/machines')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size= 500, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 0}, page_content='Citation: Hussain, M. YOLO-v1 to\\nYOLO-v8, the Rise of YOLO and Its\\nComplementary Nature toward\\nDigital Manufacturing and Industrial\\nDefect Detection. Machines 2023 ,11,\\n677. https://doi.org/10.3390/\\nmachines11070677\\nAcademic Editor: Sang Do Noh\\nReceived: 30 May 2023\\nRevised: 15 June 2023\\nAccepted: 21 June 2023\\nPublished: 23 June 2023\\nCopyright: © 2023 by the author.\\nLicensee MDPI, Basel, Switzerland.\\nThis article is an open access article\\ndistributed under the terms and'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 0}, page_content='conditions of the Creative Commons\\nAttribution (CC BY) license (https://\\ncreativecommons.org/licenses/by/\\n4.0/).\\nmachines\\nReview\\nYOLO-v1 to YOLO-v8, the Rise of YOLO and Its\\nComplementary Nature toward Digital Manufacturing and\\nIndustrial Defect Detection\\nMuhammad Hussain\\nDepartment of Computer Science, School of Computing and Engineering, University of Huddersﬁeld,\\nQueensgate, Huddersﬁeld HD1 3DH, UK; m.hussain@hud.ac.uk'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 0}, page_content='Abstract: Since its inception in 2015, the YOLO (You Only Look Once) variant of object detectors has\\nrapidly grown, with the latest release of YOLO-v8 in January 2023. YOLO variants are underpinned\\nby the principle of real-time and high-classiﬁcation performance, based on limited but efﬁcient\\ncomputational parameters. This principle has been found within the DNA of all YOLO variants\\nwith increasing intensity, as the variants evolve addressing the requirements of automated quality'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 0}, page_content='inspection within the industrial surface defect detection domain, such as the need for fast detection,\\nhigh accuracy, and deployment onto constrained edge devices. This paper is the ﬁrst to provide an\\nin-depth review of the YOLO evolution from the original YOLO to the recent release (YOLO-v8) from\\nthe perspective of industrial manufacturing. The review explores the key architectural advancements'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 0}, page_content='proposed at each iteration, followed by examples of industrial deployment for surface defect detection\\nendorsing its compatibility with industrial requirements.\\nKeywords: industrial defect detection; object detection; smart manufacturing; quality inspection\\n1. Introduction\\nHumans via the visual cortex, a primary cortical region of the brain responsible for\\nprocessing visual information [ 1], are able to observe, recognize [ 2], and differentiate'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 0}, page_content='between objects instantaneously [ 3]. Studying the inner workings of the visual cortex and\\nthe brain in general has paved the way for artiﬁcial neural networks (ANNs) [ 4] along\\nwith a myriad of computational architectures residing under the deep learning umbrella.\\nIn the last decade, owing to rapid and revolutionary advancements in the ﬁeld of deep\\nlearning [5], researchers have exerted their efforts on providing efﬁcient simulation of the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 0}, page_content='human visual system to computers, i.e., enabling computers to detect objects of interest\\nwithin static images and video [ 6], a ﬁeld known as computer vision (CV) [ 7]. CV is\\na prevalent research area for deep learning researchers and practitioners in the present\\ndecade. It is composed of subﬁelds consisting of image classiﬁcation [ 8], object detection [ 9],\\nand object segmentation [ 10]. All three ﬁelds share a common architectural theme, namely,'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 0}, page_content='manipulation of convolutional neural networks (CNNs) [ 11]. CNNs are accepted as the de\\nfacto when dealing with image data. In comparison with conventional image processing\\nand artiﬁcial defection methods, CNNs utilize multiple convolutional layers coupled with\\naggregation, i.e., pooling structures aiming to unearth deep semantic features hidden away\\nwithin the pixels of the image [12].\\nArtiﬁcial intelligence (AI) has found opportunities in industries across the spectrum'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 0}, page_content='from renewable energy [ 13,14] and security to healthcare [ 15] and the education sector.\\nHowever, one industry that is poised for signiﬁcant automation through CV is the manu-\\nfacturing industry. Quality inspection (QI) is an integral part of any manufacturing domain\\nproviding integrity and conﬁdence to the clients on the quality of the manufactured prod-\\nucts [ 16]. Manufacturing has wide scope for automation; however, when dealing with'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 0}, page_content='surface inspection [ 17], defects can take sophisticated forms [ 18], making human-based\\nMachines 2023 ,11, 677. https://doi.org/10.3390/machines11070677 https://www.mdpi.com/journal/machines'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 1}, page_content='Machines 2023 ,11, 677 2 of 25\\nquality inspection a cumbersome task with manifold inefﬁciencies linked to human bias,\\nfatigue, cost, and downtime [ 19]. These inefﬁciencies provide an opportunity for CV-based\\nsolutions to present automated quality inspection that can be integrated within existing\\nsurface defect inspection processes, increasing efﬁciency whilst overcoming bottlenecks\\npresented via conventional inspection methodologies [20].'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 1}, page_content='However, for success, CV-based architectures must conform to a stringent set of\\ndeployment requirements that can vary from one manufacturing sector to another [ 21]. In\\nthe majority of applications, the focus is not only on the determination of the defect, but also\\non multiple defects along with the locality details of each [ 22]. Therefore, object detection\\nis preferred over image classiﬁcation since the latter only focuses on determination of'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 1}, page_content='object within the image without providing any locality information. Architectures within\\nthe object detection domain can be classiﬁed into single-stage or two-stage detectors [ 23].\\nTwo-stage detectors split the detection process into two stages: Feature extraction/proposal\\nfollowed by regression and classiﬁcation for acquiring the output [ 24]. Although this can\\nprovide high accuracy, it comes with a high computational demand making it inefﬁcient for'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 1}, page_content='real-time deployment onto constrained edge devices. Single-stage detectors, on the other\\nhand, merge the two processes into one, enabling the classiﬁcation and regression via a\\nsingle pass, signiﬁcantly reduce the computational demand, and provide a more compelling\\ncase for production-based deployment [ 25]. Although many single-stage detectors have\\nbeen introduced, such as single shot detector (SSD) [ 26], deconvolutional single shot'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 1}, page_content='detector (D-SSD) [ 27], and RetinaNet [ 28], the YOLO (You Only Look Once) [ 29] family of\\narchitectures seems to be gaining high traction due to its high compatibility with industrial\\nrequirements, such as accuracy, lightweight, and edge-friendly deployment conditions.\\nThe last half-a-decade has been dominated by the introduction of YOLO variants, with the\\nmost recent variant introduced in 2022 as YOLO-v8.\\nTo the best of our knowledge, there is no cohesive review of the advancing YOLO'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 1}, page_content='variants, benchmarking technical advancements, and their implications on industrial\\ndeployment. This paper reviews the YOLO variants released to the present date, focusing\\non presenting the key technical contributions of each YOLO iteration and its impact on key\\nindustrial metrics required for deployment, such as accuracy, speed, and computational\\nefﬁcacy. As a result, the aim is to provide researchers and practitioners with a better'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 1}, page_content='understanding of the inner workings of each variant, enabling them to select the most\\nrelevant architecture based on their industrial requirements. Additionally, literature on\\nthe deployment of YOLO architectures for various industrial surface defect detection\\napplications is presented.\\nThe subsequent structure of the review is as follows. The ﬁrst section provides an\\nintroduction to single- and two-stage detectors and the anatomy for single-stage object'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 1}, page_content='detectors. Next, the evolution of YOLO variants is presented, detailing the key contributions\\nfrom YOLO-v1 to YOLO-v8, followed by a review of the literature focused on YOLO-based\\nimplementation of industrial surface defect detection. Finally, the discussion section\\nfocuses on summarizing the reviewed literature, followed by extracted conclusions, future\\ndirections, and challenges are presented.\\nObject Detection\\nCNNs can be categorized as convolution-based feed forward neural networks for'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 1}, page_content='classiﬁcation purposes [ 30]. The input layer is followed by multiple convolutional layers\\nto acquire an increased set of smaller-scale feature maps. These feature maps post further\\nmanipulation are transformed into one-dimensional feature vectors before being used as\\ninput to the fully connected layer(s). The process of feature extraction and feature map\\nmanipulation is vital to the overall accuracy of the network; therefore, this can involve the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 1}, page_content='stacking of multiple convolutional and pooling layers for richer feature maps. Popular\\narchitectures for feature extraction include AlexNet [ 31], VGGNet [ 32], GoogleNet [ 33], and\\nResNet [ 34]. AlexNet is proposed in 2012 and consists of ﬁve convolutional, three pooling,\\nand three fully connected layers primarily utilized for image classiﬁcation tasks. VGGNet'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 2}, page_content='Machines 2023 ,11, 677 3 of 25\\nfocused on performance enhancement by increasing the internal depth of the architecture,\\nintroducing several variants with increased layers, VGG-16/19. GoogleNet introduced the\\ncascading concept by cascading multiple ‘inception’ modules, whilst ResNet introduced\\nthe concept of skip-connections for preserving information and making it available from\\nthe earlier to the later layers of the architecture.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 2}, page_content='The motive for an object detector is to infer whether the object(s) of interest are\\nresiding in the image or present the frame of a video. If the object(s) of interest are\\npresent, the detector returns the respective class and locality, i.e., location dimensions\\nof the object(s). Object detection can be further divided into two sub-categories: Two-\\nstage methods and one-stage methods as shown in Figure 1. The former initiates the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 2}, page_content='ﬁrst stage with the selection of numerous proposals, then in the second stage, performs\\nprediction on the proposed regions. Examples of two-stage detectors include the famous\\nR-CNN [ 35] variants, such as Fast R-CNN [ 36] and Faster R-CNN [ 37], boasting high\\naccuracies but low computational efﬁciency. The latter transforms the task into a regression\\nproblem, eliminating the need for an initial stage dedicated to selecting candidate regions;'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 2}, page_content='therefore, the candidate selection and prediction is achieved in a single pass. As a result,\\narchitectures falling into this category are computationally less demanding, generating\\nhigher FPS and detection speed, but in general the accuracy tends to be inferior with respect\\nto two-stage detectors.\\nMachines 2023 , 11, x FOR PEER REVIEW 3 of 26 \\n \\n and ResNet [34]. AlexNet is prop osed in 2012 and consists of ﬁve convolutional, three'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 2}, page_content='pooling, and three fully connected layers primarily utilized for image classi ﬁcation tasks. \\nVGGNet focused on performance enhancement by increasing the internal depth of the \\narchitecture, introducing several variants with increased layers, VGG-16/19. GoogleNet \\nintroduced the cascading concept by cascading multiple ‘inception’ modules, whilst Res-\\nNet introduced the concept of skip-connections  for preserving information and making it'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 2}, page_content='available from the earlier to the later layers of the architecture. \\nThe motive for an object detector is to infe r whether the object(s) of interest are resid-\\ning in the image or present the frame of a video. If the object(s) of interest are present, the \\ndetector returns the respective class and locality , i.e., location dimensions of the object(s).'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 2}, page_content='Object detection can be further divided into two sub-categories: Two-stage methods and one-stage methods as shown in Figure 1. The former initiates the ﬁrst stage with the se-\\nlection of numerous proposals, then in the second stage, performs prediction on the pro-\\nposed regions. Examples of two-stage detect ors include the famous R-CNN [35] variants, \\nsuch as Fast R-CNN [36] and Faster R-CNN [37], boasting high accuracies but low com-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 2}, page_content='putational e ﬃciency. The la tter transforms the task into a regression problem, eliminating \\nthe need for an initial stage dedicated to selecting candidate regions; therefore, the candi-\\ndate selection and prediction is achieved in a single pass. As a result, architectures falling \\ninto this category are computationally less demanding, generating higher FPS and detec-\\ntion speed, but in general the accuracy tends to be inferior with respect to two-stage de-\\ntectors.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 2}, page_content='tectors. \\n \\nFigure 1. Object detector anatomy. \\n2. Original YOLO Algorithm \\nYOLO was introduced to the computer vision community via a paper release in 2015 \\nby Joseph Redmon et al. [29] titled ‘You Only Look Once: Uni ﬁed, Real-Time Object De-\\ntection’. The paper reframed object detection, presenting it essentially as a single pass re-\\ngression problem, initiating with image pixels and moving to bounding box and class'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 2}, page_content='probabilities. The proposed approach based on the ‘uni ﬁed’ concept enabled the simulta-\\nneous prediction of multiple bounding boxe s and class probabilities, improving both \\nspeed and accuracy. \\nSince its inception in 2016 until the present year (2023), the YOLO family has contin-\\nued to evolve at a rapid pace. Although the initial author (Joseph Redmon) halted further \\nwork within the computer vision domain at YOLO-v3 [38], the e ﬀectiveness and potential'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 2}, page_content='of the core ‘uni ﬁed’ concept have been further developed by several authors, with the \\nlatest addition to the YOLO family coming in  the form of YOLO-v8. Figure 2 presents the \\nYOLO evolution timeline. \\nFigure 1. Object detector anatomy.\\n2. Original YOLO Algorithm\\nYOLO was introduced to the computer vision community via a paper release in 2015 by\\nJoseph Redmon et al. [ 29] titled ‘You Only Look Once: Uniﬁed, Real-Time Object Detection’.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 2}, page_content='The paper reframed object detection, presenting it essentially as a single pass regression\\nproblem, initiating with image pixels and moving to bounding box and class probabilities.\\nThe proposed approach based on the ‘uniﬁed’ concept enabled the simultaneous prediction\\nof multiple bounding boxes and class probabilities, improving both speed and accuracy.\\nSince its inception in 2016 until the present year (2023), the YOLO family has continued'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 2}, page_content='to evolve at a rapid pace. Although the initial author (Joseph Redmon) halted further work\\nwithin the computer vision domain at YOLO-v3 [ 38], the effectiveness and potential of\\nthe core ‘uniﬁed’ concept have been further developed by several authors, with the latest\\naddition to the YOLO family coming in the form of YOLO-v8. Figure 2 presents the YOLO\\nevolution timeline.\\n2.1. Original YOLO\\nThe core principle proposed by YOLO-v1 was the imposing of a grid cell with dimen-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 2}, page_content='sions of s×s onto the image. In the case of the center of the object of interest falling into one\\nof the grid cells, that particular grid cell would be responsible for the detection of that object.\\nThis permitted other cells to disregard that object in the case of multiple appearances.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 3}, page_content='Machines 2023 ,11, 677 4 of 25\\nMachines 2023 , 11, x FOR PEER REVIEW 4 of 26 \\n \\n  \\nFigure 2. YOLO evolution timeline. \\n2.1. Original YOLO \\nThe core principle proposed by YOLO-v1 wa s the imposing of a grid cell with di-\\nmensions of s×s onto the image. In the case of  the center of the object of interest falling \\ninto one of the grid cells, that particular grid cell would be responsible for the detection'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 3}, page_content='of that object. This permi tted other cells to disregard that object in the case of multiple \\nappearances. \\nFor implementation of object detection, each grid cell would predict B bounding \\nboxes along with the dimensions and con ﬁdence scores. The con ﬁdence score was indic-\\native of the absence or presence of an object within the bounding box. Therefore, the con-\\nﬁdence score can be expressed as Equation (1): \\n𝑐𝑜𝑛𝑓𝑖𝑑𝑒𝑛𝑐𝑒 𝑠𝑐𝑜𝑟𝑒 = 𝑝 (𝑜𝑏𝑗𝑒𝑐𝑡 )∗𝐼 𝑜 𝑈\\u0be3\\u0be5\\u0bd8ௗ௧\\u0be5௨௧\\u0bdb  (1)'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 3}, page_content='where 𝑝(𝑜𝑏𝑗𝑒𝑐𝑡 ) signi ﬁed the probability of the object being present, with a range of 0–1 \\nwith 0 indicating that the object is not present and 𝐼𝑜𝑈\\u0be3\\u0be5\\u0bd8ௗ௧\\u0be5௨௧\\u0bdb represented the intersection-\\nover-union with the predicted bounding box with respect to the ground truth bounding \\nbox. \\nEach bounding box consisted of ﬁve components ( x, y, w, h, and the  conﬁdence score) \\nwith the ﬁrst four components  corresponding to center coordinates ( x, y, width, and height )'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 3}, page_content='of the respective bounding box as shown in Figure 3.  \\nFigure 2. YOLO evolution timeline.\\nFor implementation of object detection, each grid cell would predict Bbounding boxes\\nalong with the dimensions and conﬁdence scores. The conﬁdence score was indicative of\\nthe absence or presence of an object within the bounding box. Therefore, the conﬁdence\\nscore can be expressed as Equation (1):\\ncon f idence score =p(object )∗IoUtruth\\npred(1)'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 3}, page_content='pred(1)\\nwhere p(object )signiﬁed the probability of the object being present, with a range of 0–1 with\\n0 indicating that the object is not present and IoUtruth\\npredrepresented the intersection-over-\\nunion with the predicted bounding box with respect to the ground truth bounding box.\\nEach bounding box consisted of ﬁve components ( x,y,w,h,and the conﬁdence score)\\nwith the ﬁrst four components corresponding to center coordinates ( x,y,width, and height )'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 3}, page_content='of the respective bounding box as shown in Figure 3.\\nMachines 2023 , 11, x FOR PEER REVIEW 5 of 26 \\n \\n  \\nFigure 3. YOLO-v1 preliminary architecture. \\nAs alluded to earlier, the input image is split into s × s grid cells (default = 7 × 7), with \\neach cell predicting B bounding boxes, each containing ﬁve parameters and sharing pre-\\ndiction probabilities of classes ( C). Therefore, the parameter output would take the fol-\\nlowing form, expressed in (2): \\n𝑠×𝑠× (5∗𝐵+𝐶 )  (2)'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 3}, page_content='𝑠×𝑠× (5∗𝐵+𝐶 )  (2)\\nConsidering the example of YOLO network wi th each cell boundi ng box prediction \\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-eter output would be give n as expressed in (3): \\n7×7×( 5∗2+8 0 )   (3)\\nThe fundamental motive of YOLO and object detection in general is the object detec-\\ntion and localization via bounding boxes. Ther efore, two sets of bounding box vectors are'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 3}, page_content='required, i.e., vector y is the representative of ground truth and vector 𝑦ሶ is the predicted \\nvector. To address multiple bounding boxes containing no object or the same object, \\nYOLO opts for non-maximum suppression (NMS). By de ﬁning a threshold value for \\nNMS, all overlapping predic ted bounding boxes with an IoU lower than the de ﬁned NMS \\nvalue are eliminated. \\nThe original YOLO based on the Darknet framework consisted of two sub-variants.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 3}, page_content='The ﬁrst architecture comprised of 24 convolutional layers with the ﬁnal layer providing \\na connection into the ﬁrst of the two fully connected layers. Whereas the ‘Fast YOLO’ var-\\niant consisted of only nine co nvolutional layers hosting fewer ﬁlters each. Inspired by the \\ninception module in GoogleNet, a sequence of 1×1   convolutional layers was imple-\\nmented for reducing the resultant feature sp ace from the preceding layers. The prelimi-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 3}, page_content='nary architecture for YOLO-v1 is presented in Figure 3. \\nTo address the issue of multiple bounding boxes for the same object or with a con ﬁ-\\ndence score of zero, i.e., no object, the authors decided to greatly penalize predictions from bounding boxes containing objects ( 𝛾\\n\\u0bd6\\u0be2\\u0be2\\u0be5ௗ =5) and the lowest penalization for prediction \\ncontaining no object ( 𝛾\\u0be1\\u0be2\\u0be2\\u0bd5\\u0bdd =0 . 5 ). The authors calculated the loss function by taking the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 3}, page_content='sum of all bounding  box parameters ( x, y, width , height , conﬁdence score, and class prob-\\nability). As a result, the ﬁrst part of the equation computes the loss of the bounding box \\nprediction with respect to the ground tr uth bounding box based on the coordinates \\n𝑥\\u0bd6\\u0bd8\\u0be1௧\\u0bd8\\u0be5 , 𝑦\\u0bd6\\u0bd8\\u0be1௧\\u0bd8\\u0be5 . 𝕝\\u0bdc\\u0bdd\\u0be2\\u0bd5\\u0bdd is set as 1 in the case of the object residing within  𝑗௧\\u0bdb bounding box \\nprediction in 𝑖௧\\u0bdb cell; otherwise, it is set as 0. The selected, i.e., predicted bounding box'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 3}, page_content='would be tasked with predicting an object wi th the greatest IoU, as expressed in (4): \\n𝛾\\u0bd6\\u0be2\\u0be2\\u0be5ௗ ∑∑ 𝕝\\u0bdc\\u0bdd\\u0be2\\u0bd5\\u0bdd[(𝑥\\u0bdc−𝑥 పෝ)ଶ+(𝑦\\u0bdc−𝑦 పෝ)ଶ]\\u0bbb\\n\\u0bddୀ\\u0b34ௌమ\\n\\u0bdcୀ\\u0b34   (4)\\nThe next component of the loss function computes the prediction error in width and \\nheight of the bounding box, similar to the preceding compon ent. However, the scale of \\nerror in the large boxes has lesser impact co mpared to the small boxes. The normalization \\nof width and height between the range 0 and 1 indicates that their square roots increase'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 3}, page_content='Figure 3. YOLO-v1 preliminary architecture.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='Machines 2023 ,11, 677 5 of 25\\nAs alluded to earlier, the input image is split into s ×s grid cells (default = 7 ×7),\\nwith each cell predicting Bbounding boxes, each containing ﬁve parameters and sharing\\nprediction probabilities of classes ( C). Therefore, the parameter output would take the\\nfollowing form, expressed in (2):\\ns×s×(5∗B+C) (2)\\nConsidering the example of YOLO network with each cell bounding box prediction'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='set to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the parameter\\noutput would be given as expressed in (3):\\n7×7×(5∗2+80) (3)\\nThe fundamental motive of YOLO and object detection in general is the object detection\\nand localization via bounding boxes. Therefore, two sets of bounding box vectors are\\nrequired, i.e., vector y is the representative of ground truth and vector.yis the predicted'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='vector. To address multiple bounding boxes containing no object or the same object, YOLO\\nopts for non-maximum suppression (NMS). By deﬁning a threshold value for NMS, all\\noverlapping predicted bounding boxes with an IoU lower than the deﬁned NMS value\\nare eliminated.\\nThe original YOLO based on the Darknet framework consisted of two sub-variants.\\nThe ﬁrst architecture comprised of 24 convolutional layers with the ﬁnal layer providing'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='a connection into the ﬁrst of the two fully connected layers. Whereas the ‘Fast YOLO’\\nvariant consisted of only nine convolutional layers hosting fewer ﬁlters each. Inspired\\nby the inception module in GoogleNet, a sequence of 1×1convolutional layers was\\nimplemented for reducing the resultant feature space from the preceding layers. The\\npreliminary architecture for YOLO-v1 is presented in Figure 3.\\nTo address the issue of multiple bounding boxes for the same object or with a conﬁ-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='dence score of zero, i.e., no object, the authors decided to greatly penalize predictions from\\nbounding boxes containing objects ( γcoord =5) and the lowest penalization for prediction\\ncontaining no object ( γnoobj =0.5). The authors calculated the loss function by taking\\nthe sum of all bounding box parameters ( x,y,width ,height , conﬁdence score, and class\\nprobability). As a result, the ﬁrst part of the equation computes the loss of the bounding box'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='prediction with respect to the ground truth bounding box based on the coordinates xcenter ,\\nycenter .\\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \\n \\n  \\nFigure 3. YOLO-v1 preliminary architecture.  \\nAs alluded to earlier, the input image is split into s  × s grid cells (default = 7  × 7), with \\neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='lowing form, expressed in (2): \\n𝑠×𝑠×(5∗𝐵+𝐶)  (2) \\nConsidering  the example of YOLO network with each cell bounding box prediction \\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\\neter output would be given as  expressed in  (3): \\n7×7×(5∗2+80)  (3) \\nThe fundamental motive of YOLO and object detection in general is the object detec-\\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='required , i.e., vector y is the representative of ground truth  and vector 𝑦̇ is the predicted \\nvector. To address multiple bounding  boxes containing no  object or the same object, \\nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \\nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \\nvalue are eliminated.  \\nThe original YOLO based on the Dark net framework consisted of two sub -variants.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='The first architecture comprised of 24 convolutional layers with the final layer providing \\na connection  into the first of the two fully connected layers. Whereas the ‘Fast YOLO ’ var-\\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \\ninception module in GoogleNet, a sequence of 1×1  convolution al layers w as imple-\\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='nary architecture for YOLO -v1 is presented in Figure 3.  \\nTo address the issue of multiple bounding boxes for the same object or with a confi-\\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \\nbounding boxes containing objects ( 𝛾𝑐𝑜𝑜𝑟𝑑 =5) and the lowest penalization for prediction \\ncontaining no object ( 𝛾𝑛𝑜𝑜𝑏𝑗 =0.5). The authors calculated the loss function by taking the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='sum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\\nability). As a result,  the first part of the equation computes the loss of the bounding box \\nprediction with respect to the ground truth bounding box based on the coordinates \\n𝑥𝑐𝑒𝑛𝑡𝑒𝑟 , 𝑦𝑐𝑒𝑛𝑡𝑒𝑟. 𝕝𝑖𝑗𝑜𝑏𝑗 is set as 1 in the case of the object residing within  𝑗𝑡ℎ bounding box \\nprediction in  𝑖𝑡ℎ cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='would be tasked with predicting an object with the greatest IoU, as expressed  in (4): \\n𝛾𝑐𝑜𝑜𝑟𝑑 ∑ ∑ 𝕝𝑖𝑗𝑜𝑏𝑗[(𝑥𝑖−𝑥𝑖̂)2+(𝑦𝑖−𝑦𝑖̂)2]𝐵\\n𝑗=0𝑆2\\n𝑖=0   (4) \\nThe next component of the loss function computes the prediction error in width and \\nheight of the bounding box, similar to the preceding component. However, the scale of \\nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \\nof width and height between the range 0 and 1 indicates that  their square roots increase'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='obj\\nijis set as 1 in the case of the object residing within jthbounding box prediction in\\nithcell; otherwise, it is set as 0. The selected, i.e., predicted bounding box would be tasked\\nwith predicting an object with the greatest IoU, as expressed in (4):\\nγcoord∑S2\\ni=0∑B\\nj=0\\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \\n \\n  \\nFigure 3. YOLO-v1 preliminary architecture.  \\nAs alluded to earlier, the input image is split into s  × s grid cells (default = 7  × 7), with'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='each cell predicting B bounding boxes, each containing five parameters and sharing pre-\\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\\nlowing form, expressed in (2): \\n𝑠×𝑠×(5∗𝐵+𝐶)  (2) \\nConsidering  the example of YOLO network with each cell bounding box prediction \\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\\neter output would be given as  expressed in  (3): \\n7×7×(5∗2+80)  (3)'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='7×7×(5∗2+80)  (3) \\nThe fundamental motive of YOLO and object detection in general is the object detec-\\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are \\nrequired , i.e., vector y is the representative of ground truth  and vector 𝑦̇ is the predicted \\nvector. To address multiple bounding  boxes containing no  object or the same object, \\nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='NMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \\nvalue are eliminated.  \\nThe original YOLO based on the Dark net framework consisted of two sub -variants. \\nThe first architecture comprised of 24 convolutional layers with the final layer providing \\na connection  into the first of the two fully connected layers. Whereas the ‘Fast YOLO ’ var-\\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='inception module in GoogleNet, a sequence of 1×1  convolution al layers w as imple-\\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\\nnary architecture for YOLO -v1 is presented in Figure 3.  \\nTo address the issue of multiple bounding boxes for the same object or with a confi-\\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='bounding boxes containing objects ( 𝛾𝑐𝑜𝑜𝑟𝑑 =5) and the lowest penalization for prediction \\ncontaining no object ( 𝛾𝑛𝑜𝑜𝑏𝑗 =0.5). The authors calculated the loss function by taking the \\nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\\nability). As a result,  the first part of the equation computes the loss of the bounding box \\nprediction with respect to the ground truth bounding box based on the coordinates'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='𝑥𝑐𝑒𝑛𝑡𝑒𝑟 , 𝑦𝑐𝑒𝑛𝑡𝑒𝑟. 𝕝𝑖𝑗𝑜𝑏𝑗 is set as 1 in the case of the object residing within  𝑗𝑡ℎ bounding box \\nprediction in  𝑖𝑡ℎ cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box \\nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \\n𝛾𝑐𝑜𝑜𝑟𝑑 ∑ ∑ 𝕝𝑖𝑗𝑜𝑏𝑗[(𝑥𝑖−𝑥𝑖̂)2+(𝑦𝑖−𝑦𝑖̂)2]𝐵\\n𝑗=0𝑆2\\n𝑖=0   (4) \\nThe next component of the loss function computes the prediction error in width and'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='height of the bounding box, similar to the preceding component. However, the scale of \\nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \\nof width and height between the range 0 and 1 indicates that  their square roots increase \\nobj\\nij[\\n(xi−ˆxi)2+(yi−ˆyi)2]\\n(4)\\nThe next component of the loss function computes the prediction error in width and\\nheight of the bounding box, similar to the preceding component. However, the scale of'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='error in the large boxes has lesser impact compared to the small boxes. The normalization\\nof width and height between the range 0 and 1 indicates that their square roots increase\\nthe differences for smaller values to a higher degree compared to that of larger values,\\nexpressed as (5):\\nγcoord∑S2\\ni=0∑B\\nj=0\\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \\n \\n  \\nFigure 3. YOLO-v1 preliminary architecture.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='As alluded to earlier, the input image is split into s  × s grid cells (default = 7  × 7), with \\neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\\nlowing form, expressed in (2): \\n𝑠×𝑠×(5∗𝐵+𝐶)  (2) \\nConsidering  the example of YOLO network with each cell bounding box prediction \\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='eter output would be given as  expressed in  (3): \\n7×7×(5∗2+80)  (3) \\nThe fundamental motive of YOLO and object detection in general is the object detec-\\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are \\nrequired , i.e., vector y is the representative of ground truth  and vector 𝑦̇ is the predicted \\nvector. To address multiple bounding  boxes containing no  object or the same object,'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='YOLO opts for non -maximum suppression (NMS). By defining a threshold value for \\nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \\nvalue are eliminated.  \\nThe original YOLO based on the Dark net framework consisted of two sub -variants. \\nThe first architecture comprised of 24 convolutional layers with the final layer providing \\na connection  into the first of the two fully connected layers. Whereas the ‘Fast YOLO ’ var-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='iant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \\ninception module in GoogleNet, a sequence of 1×1  convolution al layers w as imple-\\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\\nnary architecture for YOLO -v1 is presented in Figure 3.  \\nTo address the issue of multiple bounding boxes for the same object or with a confi-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='dence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \\nbounding boxes containing objects ( 𝛾𝑐𝑜𝑜𝑟𝑑 =5) and the lowest penalization for prediction \\ncontaining no object ( 𝛾𝑛𝑜𝑜𝑏𝑗 =0.5). The authors calculated the loss function by taking the \\nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\\nability). As a result,  the first part of the equation computes the loss of the bounding box'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='prediction with respect to the ground truth bounding box based on the coordinates \\n𝑥𝑐𝑒𝑛𝑡𝑒𝑟 , 𝑦𝑐𝑒𝑛𝑡𝑒𝑟. 𝕝𝑖𝑗𝑜𝑏𝑗 is set as 1 in the case of the object residing within  𝑗𝑡ℎ bounding box \\nprediction in  𝑖𝑡ℎ cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box \\nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \\n𝛾𝑐𝑜𝑜𝑟𝑑 ∑ ∑ 𝕝𝑖𝑗𝑜𝑏𝑗[(𝑥𝑖−𝑥𝑖̂)2+(𝑦𝑖−𝑦𝑖̂)2]𝐵\\n𝑗=0𝑆2\\n𝑖=0   (4)'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='𝑗=0𝑆2\\n𝑖=0   (4) \\nThe next component of the loss function computes the prediction error in width and \\nheight of the bounding box, similar to the preceding component. However, the scale of \\nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \\nof width and height between the range 0 and 1 indicates that  their square roots increase \\nobj\\nij[(√wi−√\\nˆwi)2\\n+(√\\nhi−√\\nˆhi)2]\\n(5)\\nNext, the loss of the conﬁdence score is computed based on whether the object is'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 4}, page_content='present or absent with respect to the bounding box. Penalization of the object conﬁdence\\nerror is only executed by the loss function if that predictor was responsible for the ground'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='Machines 2023 ,11, 677 6 of 25\\ntruth bounding box.\\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \\n \\n  \\nFigure 3. YOLO-v1 preliminary architecture.  \\nAs alluded to earlier, the input image is split into s  × s grid cells (default = 7  × 7), with \\neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\\nlowing form, expressed in (2): \\n𝑠×𝑠×(5∗𝐵+𝐶)  (2)'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='𝑠×𝑠×(5∗𝐵+𝐶)  (2) \\nConsidering  the example of YOLO network with each cell bounding box prediction \\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\\neter output would be given as  expressed in  (3): \\n7×7×(5∗2+80)  (3) \\nThe fundamental motive of YOLO and object detection in general is the object detec-\\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='required , i.e., vector y is the representative of ground truth  and vector 𝑦̇ is the predicted \\nvector. To address multiple bounding  boxes containing no  object or the same object, \\nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \\nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \\nvalue are eliminated.  \\nThe original YOLO based on the Dark net framework consisted of two sub -variants.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='The first architecture comprised of 24 convolutional layers with the final layer providing \\na connection  into the first of the two fully connected layers. Whereas the ‘Fast YOLO ’ var-\\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \\ninception module in GoogleNet, a sequence of 1×1  convolution al layers w as imple-\\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='nary architecture for YOLO -v1 is presented in Figure 3.  \\nTo address the issue of multiple bounding boxes for the same object or with a confi-\\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \\nbounding boxes containing objects ( 𝛾𝑐𝑜𝑜𝑟𝑑 =5) and the lowest penalization for prediction \\ncontaining no object ( 𝛾𝑛𝑜𝑜𝑏𝑗 =0.5). The authors calculated the loss function by taking the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='sum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\\nability). As a result,  the first part of the equation computes the loss of the bounding box \\nprediction with respect to the ground truth bounding box based on the coordinates \\n𝑥𝑐𝑒𝑛𝑡𝑒𝑟 , 𝑦𝑐𝑒𝑛𝑡𝑒𝑟. 𝕝𝑖𝑗𝑜𝑏𝑗 is set as 1 in the case of the object residing within  𝑗𝑡ℎ bounding box \\nprediction in  𝑖𝑡ℎ cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='would be tasked with predicting an object with the greatest IoU, as expressed  in (4): \\n𝛾𝑐𝑜𝑜𝑟𝑑 ∑ ∑ 𝕝𝑖𝑗𝑜𝑏𝑗[(𝑥𝑖−𝑥𝑖̂)2+(𝑦𝑖−𝑦𝑖̂)2]𝐵\\n𝑗=0𝑆2\\n𝑖=0   (4) \\nThe next component of the loss function computes the prediction error in width and \\nheight of the bounding box, similar to the preceding component. However, the scale of \\nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \\nof width and height between the range 0 and 1 indicates that  their square roots increase'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='obj\\nijis set to 1 when the object is present in the cell; otherwise, it is set\\nas 0, whilst\\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \\n \\n  \\nFigure 3. YOLO-v1 preliminary architecture.  \\nAs alluded to earlier, the input image is split into s  × s grid cells (default = 7  × 7), with \\neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\\nlowing form, expressed in (2):'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='𝑠×𝑠×(5∗𝐵+𝐶)  (2) \\nConsidering  the example of YOLO network with each cell bounding box prediction \\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\\neter output would be given as  expressed in  (3): \\n7×7×(5∗2+80)  (3) \\nThe fundamental motive of YOLO and object detection in general is the object detec-\\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='required , i.e., vector y is the representative of ground truth  and vector 𝑦̇ is the predicted \\nvector. To address multiple bounding  boxes containing no  object or the same object, \\nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \\nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \\nvalue are eliminated.  \\nThe original YOLO based on the Dark net framework consisted of two sub -variants.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='The first architecture comprised of 24 convolutional layers with the final layer providing \\na connection  into the first of the two fully connected layers. Whereas the ‘Fast YOLO ’ var-\\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \\ninception module in GoogleNet, a sequence of 1×1  convolution al layers w as imple-\\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='nary architecture for YOLO -v1 is presented in Figure 3.  \\nTo address the issue of multiple bounding boxes for the same object or with a confi-\\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \\nbounding boxes containing objects ( 𝛾𝑐𝑜𝑜𝑟𝑑 =5) and the lowest penalization for prediction \\ncontaining no object ( 𝛾𝑛𝑜𝑜𝑏𝑗 =0.5). The authors calculated the loss function by taking the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='sum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\\nability). As a result,  the first part of the equation computes the loss of the bounding box \\nprediction with respect to the ground truth bounding box based on the coordinates \\n𝑥𝑐𝑒𝑛𝑡𝑒𝑟 , 𝑦𝑐𝑒𝑛𝑡𝑒𝑟. 𝕝𝑖𝑗𝑜𝑏𝑗 is set as 1 in the case of the object residing within  𝑗𝑡ℎ bounding box \\nprediction in  𝑖𝑡ℎ cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='would be tasked with predicting an object with the greatest IoU, as expressed  in (4): \\n𝛾𝑐𝑜𝑜𝑟𝑑 ∑ ∑ 𝕝𝑖𝑗𝑜𝑏𝑗[(𝑥𝑖−𝑥𝑖̂)2+(𝑦𝑖−𝑦𝑖̂)2]𝐵\\n𝑗=0𝑆2\\n𝑖=0   (4) \\nThe next component of the loss function computes the prediction error in width and \\nheight of the bounding box, similar to the preceding component. However, the scale of \\nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \\nof width and height between the range 0 and 1 indicates that  their square roots increase'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='noobj\\nijworks in the opposite way, as shown in (6):\\n∑S2\\ni=0∑B\\nj=0\\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \\n \\n  \\nFigure 3. YOLO-v1 preliminary architecture.  \\nAs alluded to earlier, the input image is split into s  × s grid cells (default = 7  × 7), with \\neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\\nlowing form, expressed in (2): \\n𝑠×𝑠×(5∗𝐵+𝐶)  (2)'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='𝑠×𝑠×(5∗𝐵+𝐶)  (2) \\nConsidering  the example of YOLO network with each cell bounding box prediction \\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\\neter output would be given as  expressed in  (3): \\n7×7×(5∗2+80)  (3) \\nThe fundamental motive of YOLO and object detection in general is the object detec-\\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='required , i.e., vector y is the representative of ground truth  and vector 𝑦̇ is the predicted \\nvector. To address multiple bounding  boxes containing no  object or the same object, \\nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \\nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \\nvalue are eliminated.  \\nThe original YOLO based on the Dark net framework consisted of two sub -variants.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='The first architecture comprised of 24 convolutional layers with the final layer providing \\na connection  into the first of the two fully connected layers. Whereas the ‘Fast YOLO ’ var-\\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \\ninception module in GoogleNet, a sequence of 1×1  convolution al layers w as imple-\\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='nary architecture for YOLO -v1 is presented in Figure 3.  \\nTo address the issue of multiple bounding boxes for the same object or with a confi-\\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \\nbounding boxes containing objects ( 𝛾𝑐𝑜𝑜𝑟𝑑 =5) and the lowest penalization for prediction \\ncontaining no object ( 𝛾𝑛𝑜𝑜𝑏𝑗 =0.5). The authors calculated the loss function by taking the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='sum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\\nability). As a result,  the first part of the equation computes the loss of the bounding box \\nprediction with respect to the ground truth bounding box based on the coordinates \\n𝑥𝑐𝑒𝑛𝑡𝑒𝑟 , 𝑦𝑐𝑒𝑛𝑡𝑒𝑟. 𝕝𝑖𝑗𝑜𝑏𝑗 is set as 1 in the case of the object residing within  𝑗𝑡ℎ bounding box \\nprediction in  𝑖𝑡ℎ cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='would be tasked with predicting an object with the greatest IoU, as expressed  in (4): \\n𝛾𝑐𝑜𝑜𝑟𝑑 ∑ ∑ 𝕝𝑖𝑗𝑜𝑏𝑗[(𝑥𝑖−𝑥𝑖̂)2+(𝑦𝑖−𝑦𝑖̂)2]𝐵\\n𝑗=0𝑆2\\n𝑖=0   (4) \\nThe next component of the loss function computes the prediction error in width and \\nheight of the bounding box, similar to the preceding component. However, the scale of \\nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \\nof width and height between the range 0 and 1 indicates that  their square roots increase'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='obj\\nij(ci−ˆci)2+γnoobj∑S2\\ni=0∑B\\nj=0\\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \\n \\n  \\nFigure 3. YOLO-v1 preliminary architecture.  \\nAs alluded to earlier, the input image is split into s  × s grid cells (default = 7  × 7), with \\neach cell predicting B bounding boxes, each containing five parameters and sharing pre-\\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\\nlowing form, expressed in (2): \\n𝑠×𝑠×(5∗𝐵+𝐶)  (2)'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='𝑠×𝑠×(5∗𝐵+𝐶)  (2) \\nConsidering  the example of YOLO network with each cell bounding box prediction \\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\\neter output would be given as  expressed in  (3): \\n7×7×(5∗2+80)  (3) \\nThe fundamental motive of YOLO and object detection in general is the object detec-\\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='required , i.e., vector y is the representative of ground truth  and vector 𝑦̇ is the predicted \\nvector. To address multiple bounding  boxes containing no  object or the same object, \\nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \\nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \\nvalue are eliminated.  \\nThe original YOLO based on the Dark net framework consisted of two sub -variants.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='The first architecture comprised of 24 convolutional layers with the final layer providing \\na connection  into the first of the two fully connected layers. Whereas the ‘Fast YOLO ’ var-\\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \\ninception module in GoogleNet, a sequence of 1×1  convolution al layers w as imple-\\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='nary architecture for YOLO -v1 is presented in Figure 3.  \\nTo address the issue of multiple bounding boxes for the same object or with a confi-\\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \\nbounding boxes containing objects ( 𝛾𝑐𝑜𝑜𝑟𝑑 =5) and the lowest penalization for prediction \\ncontaining no object ( 𝛾𝑛𝑜𝑜𝑏𝑗 =0.5). The authors calculated the loss function by taking the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='sum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\\nability). As a result,  the first part of the equation computes the loss of the bounding box \\nprediction with respect to the ground truth bounding box based on the coordinates \\n𝑥𝑐𝑒𝑛𝑡𝑒𝑟 , 𝑦𝑐𝑒𝑛𝑡𝑒𝑟. 𝕝𝑖𝑗𝑜𝑏𝑗 is set as 1 in the case of the object residing within  𝑗𝑡ℎ bounding box \\nprediction in  𝑖𝑡ℎ cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='would be tasked with predicting an object with the greatest IoU, as expressed  in (4): \\n𝛾𝑐𝑜𝑜𝑟𝑑 ∑ ∑ 𝕝𝑖𝑗𝑜𝑏𝑗[(𝑥𝑖−𝑥𝑖̂)2+(𝑦𝑖−𝑦𝑖̂)2]𝐵\\n𝑗=0𝑆2\\n𝑖=0   (4) \\nThe next component of the loss function computes the prediction error in width and \\nheight of the bounding box, similar to the preceding component. However, the scale of \\nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \\nof width and height between the range 0 and 1 indicates that  their square roots increase'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='noobj\\nij(xi−ˆxi)2+(ci−ˆci)2(6)\\nThe last component of the loss function, similar to the normal classiﬁcation loss,\\ncalculates the class (c) probability loss, except for the\\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \\n \\n  \\nFigure 3. YOLO-v1 preliminary architecture.  \\nAs alluded to earlier, the input image is split into s  × s grid cells (default = 7  × 7), with \\neach cell predicting B bounding boxes, each containing five parameters and sharing pre-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='diction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\\nlowing form, expressed in (2): \\n𝑠×𝑠×(5∗𝐵+𝐶)  (2) \\nConsidering  the example of YOLO network with each cell bounding box prediction \\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\\neter output would be given as  expressed in  (3): \\n7×7×(5∗2+80)  (3) \\nThe fundamental motive of YOLO and object detection in general is the object detec-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='tion and localization via bounding boxes. Therefore , two sets of bounding box vectors are \\nrequired , i.e., vector y is the representative of ground truth  and vector 𝑦̇ is the predicted \\nvector. To address multiple bounding  boxes containing no  object or the same object, \\nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for \\nNMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \\nvalue are eliminated.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='The original YOLO based on the Dark net framework consisted of two sub -variants. \\nThe first architecture comprised of 24 convolutional layers with the final layer providing \\na connection  into the first of the two fully connected layers. Whereas the ‘Fast YOLO ’ var-\\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the \\ninception module in GoogleNet, a sequence of 1×1  convolution al layers w as imple-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='mented for reducing the resultant feature space from the preceding layers.  The prelimi-\\nnary architecture for YOLO -v1 is presented in Figure 3.  \\nTo address the issue of multiple bounding boxes for the same object or with a confi-\\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from \\nbounding boxes containing objects ( 𝛾𝑐𝑜𝑜𝑟𝑑 =5) and the lowest penalization for prediction'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='containing no object ( 𝛾𝑛𝑜𝑜𝑏𝑗 =0.5). The authors calculated the loss function by taking the \\nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\\nability). As a result,  the first part of the equation computes the loss of the bounding box \\nprediction with respect to the ground truth bounding box based on the coordinates \\n𝑥𝑐𝑒𝑛𝑡𝑒𝑟 , 𝑦𝑐𝑒𝑛𝑡𝑒𝑟. 𝕝𝑖𝑗𝑜𝑏𝑗 is set as 1 in the case of the object residing within  𝑗𝑡ℎ bounding box'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='prediction in  𝑖𝑡ℎ cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box \\nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \\n𝛾𝑐𝑜𝑜𝑟𝑑 ∑ ∑ 𝕝𝑖𝑗𝑜𝑏𝑗[(𝑥𝑖−𝑥𝑖̂)2+(𝑦𝑖−𝑦𝑖̂)2]𝐵\\n𝑗=0𝑆2\\n𝑖=0   (4) \\nThe next component of the loss function computes the prediction error in width and \\nheight of the bounding box, similar to the preceding component. However, the scale of'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='error in the large boxes has lesser impact compared to the small boxes. The normalizati on \\nof width and height between the range 0 and 1 indicates that  their square roots increase \\nobj\\nijpart, expressed in (7):\\n∑S2\\ni=0\\nMachines 2023 , 11, x FOR PEER REVIEW  5 of 26 \\n \\n  \\nFigure 3. YOLO-v1 preliminary architecture.  \\nAs alluded to earlier, the input image is split into s  × s grid cells (default = 7  × 7), with'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='each cell predicting B bounding boxes, each containing five parameters and sharing pre-\\ndiction probabilities of classes ( C). Therefore,  the parameter output would take the fol-\\nlowing form, expressed in (2): \\n𝑠×𝑠×(5∗𝐵+𝐶)  (2) \\nConsidering  the example of YOLO network with each cell bounding box prediction \\nset to 2 and evaluating the benchmark COCO dataset consisting of 80 classes, the param-\\neter output would be given as  expressed in  (3): \\n7×7×(5∗2+80)  (3)'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='7×7×(5∗2+80)  (3) \\nThe fundamental motive of YOLO and object detection in general is the object detec-\\ntion and localization via bounding boxes. Therefore , two sets of bounding box vectors are \\nrequired , i.e., vector y is the representative of ground truth  and vector 𝑦̇ is the predicted \\nvector. To address multiple bounding  boxes containing no  object or the same object, \\nYOLO opts for non -maximum suppression (NMS). By defining a threshold value for'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='NMS, all overlapping predicted bounding boxes with an IoU lower than the defined NM S \\nvalue are eliminated.  \\nThe original YOLO based on the Dark net framework consisted of two sub -variants. \\nThe first architecture comprised of 24 convolutional layers with the final layer providing \\na connection  into the first of the two fully connected layers. Whereas the ‘Fast YOLO ’ var-\\niant consisted of only nine convolutional layers hosting fewer filters each. Inspired by the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='inception module in GoogleNet, a sequence of 1×1  convolution al layers w as imple-\\nmented for reducing the resultant feature space from the preceding layers.  The prelimi-\\nnary architecture for YOLO -v1 is presented in Figure 3.  \\nTo address the issue of multiple bounding boxes for the same object or with a confi-\\ndence score of zero , i.e., no object,  the authors decided to greatly penalize predictions from'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='bounding boxes containing objects ( 𝛾𝑐𝑜𝑜𝑟𝑑 =5) and the lowest penalization for prediction \\ncontaining no object ( 𝛾𝑛𝑜𝑜𝑏𝑗 =0.5). The authors calculated the loss function by taking the \\nsum of a ll bounding box parameters ( x, y, width, height, confidence score , and class prob-\\nability). As a result,  the first part of the equation computes the loss of the bounding box \\nprediction with respect to the ground truth bounding box based on the coordinates'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='𝑥𝑐𝑒𝑛𝑡𝑒𝑟 , 𝑦𝑐𝑒𝑛𝑡𝑒𝑟. 𝕝𝑖𝑗𝑜𝑏𝑗 is set as 1 in the case of the object residing within  𝑗𝑡ℎ bounding box \\nprediction in  𝑖𝑡ℎ cell; otherwise , it is set as 0. The selected , i.e., predicted bounding box \\nwould be tasked with predicting an object with the greatest IoU, as expressed  in (4): \\n𝛾𝑐𝑜𝑜𝑟𝑑 ∑ ∑ 𝕝𝑖𝑗𝑜𝑏𝑗[(𝑥𝑖−𝑥𝑖̂)2+(𝑦𝑖−𝑦𝑖̂)2]𝐵\\n𝑗=0𝑆2\\n𝑖=0   (4) \\nThe next component of the loss function computes the prediction error in width and'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='height of the bounding box, similar to the preceding component. However, the scale of \\nerror in the large boxes has lesser impact compared to the small boxes. The normalizati on \\nof width and height between the range 0 and 1 indicates that  their square roots increase \\nobj\\nij∑c∈classes\\n.(pi(c)−\\nMachines 2023 , 11, x FOR PEER REVIEW  6 of 26 \\n \\n the differences for smaller values to a higher degree compared to that of larger values, \\nexpressed as (5):  \\n𝛾𝑐𝑜𝑜𝑟𝑑 ∑ ∑ 𝕝𝑖𝑗𝑜𝑏𝑗[(√𝑤𝑖−√𝑤𝑖̂)2+(√ℎ𝑖−√ℎ𝑖̂)2'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content=']𝐵\\n𝑗=0𝑆2\\n𝑖=0   (5) \\nNext, the loss of the confidence score is computed based on whether the object is \\npresent or absent with respect to the bounding box. Penalization of the object confidence \\nerror is only executed by the loss function if that predictor was responsible for the ground \\ntruth bounding box. 𝕝𝑖𝑗𝑜𝑏𝑗 is set to 1 when the object is present in the cell ; otherwise , it is \\nset as 0, whilst 𝕝ijnoobj works in the opposite way, as shown in (6):'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='∑ ∑ 𝕝ijobj(ci−cî)2+γnoobj ∑ ∑ 𝕝ijnoobj(xi−xî)2+(ci−cî)2 B\\nj=0S2\\ni=0B\\nj=0S2\\ni=0   (6) \\n  \\nThe last component of the loss function, similar to the normal classification loss, cal-\\nculates the class ( c) probability loss, except for the 𝕝𝑖𝑗𝑜𝑏𝑗 part, expressed in (7):  \\n \\n∑ 𝕝𝑖𝑗𝑜𝑏𝑗 𝑆2\\n𝑖=0 ∑ (𝑝𝑖(𝑐)− 𝑐∈𝑐𝑙𝑎𝑠𝑠𝑒𝑠.𝑝𝑖(𝑐)̂ )2  (7) \\n  \\nPerformance wise, the simple YOLO (24 conv olutional  layers) when trained on the \\nPASCAL VOC dataset (2007 and 2012) [39,40] achieved a mean average precision (refer-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='ring to cross -class performance) (mAP) of 63.4% at 45 FPS, whilst Fast YOLO achieved \\n52.7% mAP at an impressive 155 FPS. Although the performance was better than real -time \\ndetectors , such as DPM -v5 [41] (33% mAP), it was lower than the state -of-the-art (SOTA) \\nat the time , i.e., Faster R -CNN (71% mAP).  \\nThere were some clear loopholes that required attention , such as the architecture hav-\\ning comparatively low recall and higher localization error compared to Faster R -CNN.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='Additionally, the architecture struggled to detect close proximity objec ts due to the fact \\nthat each grid cell was capped to two bounding box proposals.  The loopholes attributed \\nto the original YOLO provided inspiration for the following variants of YOLO.  \\n2.2. YOLO-v2/9000  \\nYOLO-v2/9000 was introduced by Joseph Redmon in 2016 [ 42]. The motive was to \\nremove or at least mitigate the inefficiencies observed with the original YOLO whil e main-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='taining the impressive speed factor. Several enhancements were claimed through the im-\\nplementation of various techniques. Batch normalization [43 ] was introduced with the in-\\nternal architecture to improve model convergence, leading to faster training. This intro-\\nduction eliminated the need for other regularization techniques , such as dropout [44] \\naimed at reducing overfitting [45]. Its effectiveness can be gauged by the fact that simply'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='introducing batch normalization improved the mAP by 2% compared to the original \\nYOLO.  \\nThe original YOLO worked with an input image size of 224  × 224 pixels during the \\ntraining stage, whilst for the detection phase , input images could be scaled up to 448  × 448 \\npixels, enforcing the architecture to adjust to the varying image resolution , which in turn \\ndecrease the mAP. To address this, the authors trained the architecture on 448  × 448 pixel'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='images for 10 epochs on the  ImageNet [46] dataset, providing the architecture with the \\ncapacity to adjust the internal filters when dealing with higher resolution images, result-\\ning in an increased mAP of 4%. Whilst architectures , such as Fast and Faster R -CNN pre-\\ndict coordinates dir ectly from the convolutional network, the original YOLO utilized fully  (7)\\nPerformance wise, the simple YOLO (24 convolutional layers) when trained on the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='PASCAL VOC dataset (2007 and 2012) [ 39,40] achieved a mean average precision (referring\\nto cross-class performance) (mAP) of 63.4% at 45 FPS, whilst Fast YOLO achieved 52.7%\\nmAP at an impressive 155 FPS. Although the performance was better than real-time\\ndetectors, such as DPM-v5 [ 41] (33% mAP), it was lower than the state-of-the-art (SOTA) at\\nthe time, i.e., Faster R-CNN (71% mAP).\\nThere were some clear loopholes that required attention, such as the architecture'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='having comparatively low recall and higher localization error compared to Faster R-CNN.\\nAdditionally, the architecture struggled to detect close proximity objects due to the fact that\\neach grid cell was capped to two bounding box proposals. The loopholes attributed to the\\noriginal YOLO provided inspiration for the following variants of YOLO.\\n2.2. YOLO-v2/9000\\nYOLO-v2/9000 was introduced by Joseph Redmon in 2016 [ 42]. The motive was'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='to remove or at least mitigate the inefﬁciencies observed with the original YOLO while\\nmaintaining the impressive speed factor. Several enhancements were claimed through\\nthe implementation of various techniques. Batch normalization [ 43] was introduced with\\nthe internal architecture to improve model convergence, leading to faster training. This\\nintroduction eliminated the need for other regularization techniques, such as dropout [ 44]'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='aimed at reducing overﬁtting [ 45]. Its effectiveness can be gauged by the fact that simply\\nintroducing batch normalization improved the mAP by 2% compared to the original YOLO.\\nThe original YOLO worked with an input image size of 224 ×224 pixels during\\nthe training stage, whilst for the detection phase, input images could be scaled up to\\n448×448 pixels, enforcing the architecture to adjust to the varying image resolution,'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='which in turn decrease the mAP . To address this, the authors trained the architecture on\\n448×448 pixel images for 10 epochs on the ImageNet [ 46] dataset, providing the architec-\\nture with the capacity to adjust the internal ﬁlters when dealing with higher resolution\\nimages, resulting in an increased mAP of 4%. Whilst architectures, such as Fast and Faster\\nR-CNN predict coordinates directly from the convolutional network, the original YOLO'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='utilized fully connected layers to serve this purpose. YOLO-v2 replaced the fully connected\\nlayer responsible for predicting bounding boxes by adding anchor boxes for bounding\\nbox predictions. Anchor boxes [ 47] are essentially a list of predeﬁned dimensions (boxes)\\naimed at best matching the objects of interest. Rather than manual determination of best-ﬁt\\nanchor boxes, the authors utilized k-means clustering [ 48] on the training set bounding'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='boxes, inclusive of the ground truth bounding boxes, grouping similar shapes and plotting\\naverage IoU with respect to the closest centroid as shown in Figure 4. YOLO-v2 was trained\\non different architectures, namely, VGG-16 and GoogleNet, in addition to the authors\\nproposing the Darknet-19 [ 49] architecture due to characteristics, such as reduced process-\\ning requirements, i.e., 5.58 FLOPs compared to 30.69 FLOPs and 8.52 FLOPs on VGG-16'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 5}, page_content='and GoogleNet, respectively. In terms of performance, YOLO-v2 provided 76.8 mAP at\\n67 FPS and 78.6 mAP at 40 FPS. The results demonstrated the architectures’ superiority\\nover SOTA architectures of that time, such as SSD and Faster R-CNN. YOLO-9000 utilized'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 6}, page_content='Machines 2023 ,11, 677 7 of 25\\nYOLO-v2 architecture, aimed at real-time detection of more than 9000 different objects;\\nhowever, at a signiﬁcantly reduced mAP of 19.7%.\\nMachines 2023 , 11, x FOR PEER REVIEW 7 of 26 \\n \\n connected layers to serve this purpose. YO LO-v2 replaced the fully connected layer re-\\nsponsible for predicting bounding boxes by adding anchor boxes for bounding box pre-\\ndictions. Anchor boxes [47] are essentially a list of prede ﬁned dimensions (boxes) aimed'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 6}, page_content='at best matching the objects of interest. Rather than manual determination of best- ﬁt an-\\nchor boxes, the authors utilized k-means clus tering [48] on the training set bounding \\nboxes, inclusive of the ground truth boundi ng boxes, grouping similar shapes and plo tting \\naverage IoU with respect to the closest cent roid as shown in Figure 4. YOLO-v2 was \\ntrained on di ﬀerent architectures, namely, VGG-16 and GoogleNet, in addition to the au-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 6}, page_content='thors proposing the Darknet-19 [49] architectu re due to characteristics, such as reduced \\nprocessing requirements, i.e., 5.58 FLOPs compared to 30.69 FLOPs and 8.52 FLOPs on \\nVGG-16 and GoogleNet, respectively. In term s of performance, YOLO-v2 provided 76.8 \\nmAP at 67 FPS and 78.6 mAP at 40 FPS. The results demonstrated the architectures’ supe-\\nriority over SOTA architectures of that time, such as SSD and Faster R-CNN. YOLO-9000'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 6}, page_content='utilized YOLO-v2 architecture, aimed at real-time detection of more than 9000 di ﬀerent \\nobjects; however, at a signi ﬁcantly reduced mAP of 19.7%. \\n \\nFigure 4. Dimension cluste rs vs. mAP. \\n2.3. YOLO-v3 \\nArchitectures, such as VGG, focused their development work around the concept \\nthat deeper networks, i.e., more internal laye rs, equated to higher accuracy. YOLO-v2 also \\nhad higher number of convolutional layers compared to its predecessor.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 6}, page_content='However, as the image progressed throug h the network, the progressive down sam-\\npling resulted in the loss of ﬁne-grained features; therefore, YOLO-v2 often struggled with \\ndetecting smaller objects. At the time research was active in addressing this issue, as evi-\\ndent by the deployment of skip connection s [50] embedded within the proposed ResNet \\narchitecture, the focus was on addressing the vanishing gradient issue by facilitating in-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 6}, page_content='formation propagation via skip connection, as presented in Figure 5. \\nFigure 4. Dimension clusters vs. mAP .\\n2.3. YOLO-v3\\nArchitectures, such as VGG, focused their development work around the concept that\\ndeeper networks, i.e., more internal layers, equated to higher accuracy. YOLO-v2 also had\\nhigher number of convolutional layers compared to its predecessor.\\nHowever, as the image progressed through the network, the progressive down sam-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 6}, page_content='pling resulted in the loss of ﬁne-grained features; therefore, YOLO-v2 often struggled\\nwith detecting smaller objects. At the time research was active in addressing this issue,\\nas evident by the deployment of skip connections [ 50] embedded within the proposed\\nResNet architecture, the focus was on addressing the vanishing gradient issue by facilitating\\ninformation propagation via skip connection, as presented in Figure 5.\\nMachines 2023 , 11, x FOR PEER REVIEW 8 of 26'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 6}, page_content='Figure 5. Skip-connection con ﬁguration. \\nYOLO-v3 proposed a hybrid architecture fa ctoring in aspects of YOLO-v2, Darknet-\\n53 [51], and the ResNet concept of residual networks. This enabled the preservation of \\nﬁne-grained features by a llowing for the gradient ﬂow from shallow layers to deeper lay-\\ners. \\nOn top of the existing 53 layers of Darknet-53 for feature extraction, a stack of 53 \\nadditional layers was added for the detection head, totaling 106 convolutional layers for'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 6}, page_content='the YOLO-v3. Additionally, YOLO-v3 facilitated multi-scale detection, namely, the archi-\\ntecture made predictions at three di ﬀerent scales of granularity for outpu tting better per-\\nformance, increasing the probability of small object detection. \\n2.4. YOLO-v4 \\nYOLO-v4 was the ﬁrst variant of the YOLO family after the original author discon-\\ntinued further work that was introduced to the computer vi sion community in April 2020'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 6}, page_content='by Alexey Bochkovsky et al. [52]. YOLO-v4 was essentially the distillation of a large suite \\nof object detection techniques, tested and en hanced for providing a real-time, lightweight \\nobject detector. \\nThe backbone of an object detector has a critical role in the quality of features ex-\\ntracted. In-line with the experimental spirit, the authors experimented with three di ﬀerent \\nbackbones: CSPResNext-50, CSPDarknet-53, and E ﬃcientNet-B3 [53]. The ﬁrst was based'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 6}, page_content='on DenseNet [54] aimed at alleviating the vanishing gradient problem and bolstering fea-ture propagation and reuse, resulting in reduced number of network parameters. E ﬃ-\\ncientNet was proposed by Google Brain. The paper posits that an optima selection for \\nparameters when scaling CNNs can be asce rtained through a search mechanism. After \\nexperimenting with the above feature extractors, the authors based on their intuition and'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 6}, page_content='backed by their experimental result s selected CSPDarknet-53 as the o ﬃcial backbone for \\nYOLO-v4. \\nFor feature aggregation, the authors experi mented with several techniques for inte-\\ngration at the neck level including feature pyramid network (FPN) [55] and path aggrega-tion network (PANet) [56]. Ultimately, the authors opted for PANet as the feature aggre-\\ngator. The modi ﬁed PANet, as shown in Figure 6, utilized the concatenation mechanism.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 6}, page_content='PANet can be seen as an advanced version of FPN, namely, PANet proposed a bo ttom-up \\naugmentation path along with the top-down path (FPN), adding a ‘shortcut’ connection \\nfor linking ﬁne-grained features from high- and low-level layers. Additionally, the authors \\nintroduced a SPP [57] block post CSPDarknet-53 aimed at increasing the receptive ﬁeld \\nand separation of the important features arriving from the backbone. \\nFigure 5. Skip-connection conﬁguration.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 7}, page_content='Machines 2023 ,11, 677 8 of 25\\nYOLO-v3 proposed a hybrid architecture factoring in aspects of YOLO-v2, Darknet-\\n53 [51], and the ResNet concept of residual networks. This enabled the preservation of\\nﬁne-grained features by allowing for the gradient ﬂow from shallow layers to deeper layers.\\nOn top of the existing 53 layers of Darknet-53 for feature extraction, a stack of 53 addi-\\ntional layers was added for the detection head, totaling 106 convolutional layers for the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 7}, page_content='YOLO-v3. Additionally, YOLO-v3 facilitated multi-scale detection, namely, the architecture\\nmade predictions at three different scales of granularity for outputting better performance,\\nincreasing the probability of small object detection.\\n2.4. YOLO-v4\\nYOLO-v4 was the ﬁrst variant of the YOLO family after the original author discon-\\ntinued further work that was introduced to the computer vision community in April 2020'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 7}, page_content='by Alexey Bochkovsky et al. [52]. YOLO-v4 was essentially the distillation of a large suite\\nof object detection techniques, tested and enhanced for providing a real-time, lightweight\\nobject detector.\\nThe backbone of an object detector has a critical role in the quality of features extracted.\\nIn-line with the experimental spirit, the authors experimented with three different back-\\nbones: CSPResNext-50, CSPDarknet-53, and EfﬁcientNet-B3 [ 53]. The ﬁrst was based on'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 7}, page_content='DenseNet [ 54] aimed at alleviating the vanishing gradient problem and bolstering feature\\npropagation and reuse, resulting in reduced number of network parameters. EfﬁcientNet\\nwas proposed by Google Brain. The paper posits that an optima selection for parameters\\nwhen scaling CNNs can be ascertained through a search mechanism. After experimenting\\nwith the above feature extractors, the authors based on their intuition and backed by their'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 7}, page_content='experimental results selected CSPDarknet-53 as the ofﬁcial backbone for YOLO-v4.\\nFor feature aggregation, the authors experimented with several techniques for integra-\\ntion at the neck level including feature pyramid network (FPN) [ 55] and path aggregation\\nnetwork (PANet) [ 56]. Ultimately, the authors opted for PANet as the feature aggregator.\\nThe modiﬁed PANet, as shown in Figure 6, utilized the concatenation mechanism. PANet'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 7}, page_content='can be seen as an advanced version of FPN, namely, PANet proposed a bottom-up augmen-\\ntation path along with the top-down path (FPN), adding a ‘shortcut’ connection for linking\\nﬁne-grained features from high- and low-level layers. Additionally, the authors introduced\\na SPP [ 57] block post CSPDarknet-53 aimed at increasing the receptive ﬁeld and separation\\nof the important features arriving from the backbone.\\nMachines 2023 , 11, x FOR PEER REVIEW 9 of 26'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 7}, page_content='Figure 6. Path aggregation. ( a) Original PAN, ( b) modi ﬁed PAN. \\nThe authors also introduced a bag-of-freebie s, presented in Figure 7, primarily con-\\nsisting of augmentations, such as Mosaic aimed at improving performance without intro-\\nducing additional baggage onto the inference time. CIoU loss [58] was also introduced as \\na freebie, focused on the overlap of the predic ted and ground truth bounding box. In the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 7}, page_content='case of no overlap, the idea was to observe the closeness of the two boxes and encourage \\noverlap if in close proximity. \\nIn addition to the bag-of-freebies, the auth ors introduced ‘bag-of-specials’, with the \\nauthors claiming that although this set of op timization techniques presented in Figure 7 \\nwould marginally impact the inference time, they would signi ﬁcantly improve the overall \\nperformance. One of the components within th e ‘bag-of-specials’ was the Mish [59] acti-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 7}, page_content='vation function aimed at moving feature creations toward their respective optimal points. Cross mini-batch normalization [60] was also  presented facilitating the running on any \\nGPU as many batch normalization techniques involve multiple GPUs operating in tan-\\ndem. \\n \\nFigure 7. State-of-the-art optimization methodologies experimented in YOLO-v4 via bag-of-spe-\\ncials. \\n2.5. YOLO-v5 \\nThe YOLO network in essence consists of th ree key pillars, namely, backbone for fea-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 7}, page_content='ture extraction, neck focused on feature aggregation, and the head for consuming output \\nFigure 6. Path aggregation. ( a) Original PAN, ( b) modiﬁed PAN.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 8}, page_content='Machines 2023 ,11, 677 9 of 25\\nThe authors also introduced a bag-of-freebies, presented in Figure 7, primarily consist-\\ning of augmentations, such as Mosaic aimed at improving performance without introducing\\nadditional baggage onto the inference time. CIoU loss [ 58] was also introduced as a freebie,\\nfocused on the overlap of the predicted and ground truth bounding box. In the case of no\\noverlap, the idea was to observe the closeness of the two boxes and encourage overlap if in\\nclose proximity.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 8}, page_content='close proximity.\\nMachines 2023 , 11, x FOR PEER REVIEW 9 of 26 \\n \\n  \\nFigure 6. Path aggregation. ( a) Original PAN, ( b) modi ﬁed PAN. \\nThe authors also introduced a bag-of-freebie s, presented in Figure 7, primarily con-\\nsisting of augmentations, such as Mosaic aimed at improving performance without intro-\\nducing additional baggage onto the inference time. CIoU loss [58] was also introduced as \\na freebie, focused on the overlap of the predic ted and ground truth bounding box. In the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 8}, page_content='case of no overlap, the idea was to observe the closeness of the two boxes and encourage \\noverlap if in close proximity. \\nIn addition to the bag-of-freebies, the auth ors introduced ‘bag-of-specials’, with the \\nauthors claiming that although this set of op timization techniques presented in Figure 7 \\nwould marginally impact the inference time, they would signi ﬁcantly improve the overall \\nperformance. One of the components within th e ‘bag-of-specials’ was the Mish [59] acti-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 8}, page_content='vation function aimed at moving feature creations toward their respective optimal points. Cross mini-batch normalization [60] was also  presented facilitating the running on any \\nGPU as many batch normalization techniques involve multiple GPUs operating in tan-\\ndem. \\n \\nFigure 7. State-of-the-art optimization methodologies experimented in YOLO-v4 via bag-of-spe-\\ncials. \\n2.5. YOLO-v5 \\nThe YOLO network in essence consists of th ree key pillars, namely, backbone for fea-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 8}, page_content='ture extraction, neck focused on feature aggregation, and the head for consuming output \\nFigure 7. State-of-the-art optimization methodologies experimented in YOLO-v4 via bag-of-specials.\\nIn addition to the bag-of-freebies, the authors introduced ‘bag-of-specials’, with the\\nauthors claiming that although this set of optimization techniques presented in Figure 7\\nwould marginally impact the inference time, they would signiﬁcantly improve the overall'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 8}, page_content='performance. One of the components within the ‘bag-of-specials’ was the Mish [ 59] acti-\\nvation function aimed at moving feature creations toward their respective optimal points.\\nCross mini-batch normalization [ 60] was also presented facilitating the running on any\\nGPU as many batch normalization techniques involve multiple GPUs operating in tandem.\\n2.5. YOLO-v5\\nThe YOLO network in essence consists of three key pillars, namely, backbone for'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 8}, page_content='feature extraction, neck focused on feature aggregation, and the head for consuming\\noutput features from the neck as input and generating detections. YOLO-v5 [ 61] similar to\\nYOLO-v4, with respect to contributions, focus on the conglomeration and reﬁnement of\\nvarious computer vision techniques for enhancing performance. In addition, in less than\\n2 months after the release of YOLO-v4, Glenn Jocher open-sourced an implementation of\\nYOLO-v5 [61].'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 8}, page_content='YOLO-v5 [61].\\nA notable mention is that YOLO-v5 was the ﬁrst native release of architectures be-\\nlonging to the YOLO clan, to be written in PyTorch [ 62] rather than Darknet. Although\\nDarknet is considered as a ﬂexible low-level research framework, it was not purpose built\\nfor production environments with a signiﬁcantly smaller number of subscribers due to\\nconﬁgurability challenges. PyTorch, on the other hand, provided an established eco-system,'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 8}, page_content='with a wider subscription base among the computer vision community and provided the\\nsupporting infrastructure for facilitating mobile device deployment.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 9}, page_content='Machines 2023 ,11, 677 10 of 25\\nIn addition, another notable proposal was the ‘automated anchor box learning’ concept.\\nIn YOLO-v2, the anchor box mechanism was introduced based on selecting anchor boxes\\nthat closely resemble the dimensions of the ground truth boxes in the training set via\\nk-means. The authors select the ﬁve close-ﬁt anchor boxes based on the COCO dataset [ 63]\\nand implement them as default boxes. However, the application of this methodology to a'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 9}, page_content='unique dataset with signiﬁcant object differentials compared to those present in the COCO\\ndataset can quickly expose the inability of the predeﬁned boxes to adapt quickly to the\\nunique dataset. Therefore, authors in YOLO-v5 integrated the anchor box selection process\\ninto the YOLO-v5 pipeline. As a result, the network would automatically learn the best-ﬁt\\nanchor boxes for the particular dataset and utilize them during training to accelerate the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 9}, page_content='process. YOLO-v5 comes in several variants with respect to the computational parameters\\nas presented in Table 1.\\nTable 1. YOLO-v5 internal variant comparison.\\nModel Average Precision (@50) Parameters FLOPs\\nYOLO-v5s 55.8% 7.5 M 13.2B\\nYOLO-v5m 62.4% 21.8 M 39.4B\\nYOLO-v5l 65.4% 47.8 M 88.1B\\nYOLO-v5x 66.9% 86.7 M 205.7B\\nYOLO-v5 comprised of a weight ﬁle equating to 27 MB compared to YOLO-v5l at 192\\nMB. Figure 8 demonstrates the superiority of YOLO-v5 over EfﬁcientDet [64].'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 9}, page_content='Machines 2023 , 11, x FOR PEER REVIEW 10 of 26 \\n \\n features from the neck as input and generating detections. YOLO-v5 [61] similar to YOLO-\\nv4, with respect to contributions, focus on the conglomeration and re ﬁnement of various \\ncomputer vision techniques for enhancing perfor mance. In addition, in less than 2 months \\nafter the release of YOLO-v4, Glenn Jocher open-sourced an implementation of YOLO-v5 \\n[61]. \\nA notable mention is that YOLO-v5 was the ﬁrst native release of architectures be-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 9}, page_content='longing to the YOLO clan, to be wri tten in PyTorch [62] rather than Darknet. Although \\nDarknet is considered as a ﬂexible low-level research framework, it was not purpose built \\nfor production environments with a signi ﬁcantly smaller number of subscribers due to \\nconﬁgurability challenges. PyTorch, on the othe r hand, provided an established eco-sys-\\ntem, with a wider subscription base among the computer vision community and provided'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 9}, page_content='the supporting infrastructure for facilitating mobile device deployment. \\nIn addition, another notable proposal was the ‘automated anchor box learning’ con-\\ncept. In YOLO-v2, the anchor box mechanism was introduced based on selecting anchor \\nboxes that closely resemble the dimensions of  the ground truth boxes in the training set \\nvia k-means. The authors select the ﬁve close- ﬁt anchor boxes based on the COCO dataset'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 9}, page_content='[63] and implement them as default boxes. However, the application of this methodology \\nto a unique dataset with signi ﬁcant object di ﬀerentials compared to those present in the \\nCOCO dataset can quickly expose the inability of the prede ﬁned boxes to adapt quickly \\nto the unique dataset. Therefore, authors in  YOLO-v5 integrated the anchor box selection'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 9}, page_content='process into the YOLO-v5 pipeline. As a result, the network would automatically learn the best- ﬁt anchor boxes for the particular dataset and utilize them during training to ac-\\ncelerate the process. YOLO-v5 comes in severa l variants with respect to the computational \\nparameters as presented in Table 1. \\nTable 1. YOLO-v5 internal variant comparison. \\nModel Average Precision (@50) Parameters FLOPs \\nYOLO-v5s 55.8% 7.5 M 13.2B \\nYOLO-v5m 62.4% 21.8 M 39.4B \\nYOLO-v5l 65.4% 47.8 M 88.1B'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 9}, page_content='YOLO-v5x 66.9% 86.7 M 205.7B \\nYOLO-v5 comprised of a weight ﬁle equating to 27 MB compared to YOLO-v5l at 192 \\nMB. Figure 8 demonstrates the superiority of YOLO-v5 over E ﬃcientDet [64]. \\n \\nFigure 8. YOLO-v5 variant comparison vs. E ﬃcientDet [61]. \\nFigure 8. YOLO-v5 variant comparison vs. EfﬁcientDet [61].\\n2.6. YOLO-v6\\nThe initial codebase for YOLO-v6 [ 65] was released in June 2022 by the Meituan\\nTechnical Team based in China. The authors focused their design strategy on producing an'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 9}, page_content='industry-orientated object detector.\\nTo meet industrial application requirements, the architecture would need to be highly\\nperformant on a range of hardware options, maintaining high speed and accuracy. To\\nconform with the diverse set of industrial applications, YOLO-v6 comes in several variants\\nstarting with YOLO-v6-nano as the fastest with the least number of parameters and reaching\\nYOLO-v6-large with high accuracy at the expense of speed, as shown in Table 2.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 10}, page_content='Machines 2023 ,11, 677 11 of 25\\nTable 2. YOLO-v6 variant comparison.\\nVariantmAP 0.5:0.95\\n(COCO-val)FPS Tesla T4 Parameters (Million)\\nYOLO-v6-N 35.9 (300 epochs) 802 4.3\\nYOLO-v6-T 40.3 (300 epochs) 449 15.0\\nYOLO-v6-RepOpt 43.3 (300 epochs) 596 17.2\\nYOLO-v6-S 43.5 (300 epochs) 495 17.2\\nYOLO-v6-M 49.7 233 34.3\\nYOLO-v6-L-ReLU 51.7 149 58.5\\nThe impressive performance presented in Table 2 is a result of several innovations'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 10}, page_content='integrated into the YOLO-v6 architecture. The key contributions can be summed into four\\npoints. First, in contrast to its predecessors, YOLO-v6 opts for an anchor-free approach,\\nmaking it 51% faster when compared to anchor-based approaches.\\nSecond, the authors introduced a revised reparametrized backbone and neck, proposed\\nas EfﬁcientRep backbone and Rep-PAN neck [ 66], namely, up to and including YOLO-v5,\\nthe regression and classiﬁcation heads shared the same features. Breaking the convention,'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 10}, page_content='YOLO-v6 implements the decoupled head as shown in Figure 9. As a result, the architecture\\nhas additional layers separating features from the ﬁnal head, as empirically shown to\\nimprove the performance. Third, YOLO-v6 mandates a two-loss function. Varifocal loss\\n(VFL) [ 67] is used as the classiﬁcation loss and distribution focal loss (DFL) [ 68], along with\\nSIoU/GIoU [ 69] as regression loss. VFL being a derivative of focal loss, treats positive'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 10}, page_content='and negative samples at varying degrees of importance, helping in balancing the learning\\nsignals from both sample types. DFL is deployed for box regression in YOLO-v6 medium\\nand large variants, treating the continuous distribution of the box locations as discretized\\nprobability distribution, which is shown to be particularly efﬁcient when the ground truth\\nbox boundaries are blurred.\\nMachines 2023 , 11, x FOR PEER REVIEW 11 of 26 \\n \\n 2.6. YOLO-v6'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 10}, page_content='2.6. YOLO-v6 \\nThe initial codebase for YOLO-v6 [65] was released in June 2022 by the Meituan Tech-\\nnical Team based in China. The authors focu sed their design strategy on producing an \\nindustry-orientated object detector. \\nTo meet industrial application requirements, the architecture would need to be \\nhighly performant on a range of hardware op tions, maintaining high speed and accuracy. \\nTo conform with the diverse set of industrial applications, YOLO-v6 comes in several var-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 10}, page_content='iants starting with YOLO-v6-nano as the fastest with the least number of parameters and reaching YOLO-v6-large with high accuracy at the expense of speed, as shown in Table 2. \\nTable 2. YOLO-v6 variant comparison. \\nVariant mAP 0.5:0.95 \\n(COCO-val) FPS Tesla T4 Parameters (Million) \\nYOLO-v6-N 35.9 (300 epochs) 802 4.3 \\nYOLO-v6-T 40.3 (300 epochs) 449 15.0 \\nYOLO-v6-RepOpt 43.3 (300 epochs) 596 17.2 \\nYOLO-v6-S 43.5 (300 epochs) 495 17.2 \\nYOLO-v6-M 49.7 233 34.3 \\nYOLO-v6-L-ReLU 51.7 149 58.5'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 10}, page_content='The impressive performance presented in Ta ble 2 is a result of several innovations \\nintegrated into the YOLO-v6 ar chitecture. The key contributi ons can be summed into four \\npoints. First, in contrast to its predecesso rs, YOLO-v6 opts for an anchor-free approach, \\nmaking it 51% faster when compared to anchor-based approaches. \\nSecond, the authors introduced a revised reparametrized backbone and neck, pro-\\nposed as E ﬃcientRep backbone and Rep-PAN neck [66], namely, up to and including'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 10}, page_content='YOLO-v5, the regression and classi ﬁcation heads shared the same features. Breaking the \\nconvention, YOLO-v6 implements the decoupled head as shown in Figure 9. As a result, \\nthe architecture has additional layers separating features from the ﬁnal head, as empiri-\\ncally shown to improve the performance. Thir d, YOLO-v6 mandates a two-loss function. \\nVarifocal loss (VFL) [67] is used as the classi ﬁcation loss and distribution focal loss (DFL)'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 10}, page_content='[68], along with SIoU/GIoU [69] as regression loss. VFL being a derivative of focal loss, \\ntreats positive and negative samples at varying degrees of importance, helping in balanc-\\ning the learning signals from both sample types. DFL is deployed for box regression in YOLO-v6 medium and large variants, treating the continuous distribution of the box lo-\\ncations as discretized probability distribution, which is shown to be particularly e ﬃcient \\nwhen the ground truth box boundaries are blurred.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 10}, page_content='Figure 9. YOLO-v6 model base architecture.\\nAdditional improvements focused on industrial applications include the use of knowl-\\nedge distillation [70], involving a teacher model used for training a student model, where\\nthe predictions of the teacher are used as soft labels along with the ground truth for training\\nthe student. This comes without fueling the computational cost as essentially the aim is\\nto train a smaller (student) model to replicate the high performance of the larger (teacher)'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 10}, page_content='model. Comparing the performance of YOLO-v6 with its predecessors, including YOLO-v5\\non the benchmark COCO dataset in Figure 10, it is clear that YOLO-v6 achieves a higher\\nmAP at various FPS.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 11}, page_content='Machines 2023 ,11, 677 12 of 25\\nMachines 2023 , 11, x FOR PEER REVIEW 12 of 26 \\n \\n Figure 9. YOLO-v6 model base architecture. \\nAdditional improvements focused on indu strial applications include the use of \\nknowledge distillation [70], involving a teacher model used for training a student model, \\nwhere the predictions of the teacher are used as soft labels along with the ground truth'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 11}, page_content='for training the student. This comes without fueling the computational cost as essentially the aim is to train a smaller (student) model to replicate the high performance of the larger \\n(teacher) model. Comparing the performance of YOLO-v6 with its predecessors, includ-\\ning YOLO-v5 on the benchmark COCO dataset in Figure 10, it is clear that YOLO-v6 \\nachieves a higher mAP at various FPS. \\n \\nFigure 10. Relative evaluation of YOLO-v6 vs. YOLO-v5 [71]. \\n2.7. YOLO-v7'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 11}, page_content='2.7. YOLO-v7 \\nThe following month after the release of YOLO-v6, the YOLO-v7 was released [72]. \\nAlthough other variants have been releas ed in between, including YOLO-X [73] and \\nYOLO-R [74], these focused more on GPU speed  enhancements with respect to inferenc-\\ning. YOLO-v7 proposes several architectural reforms for improving the accuracy and \\nmaintaining high detection speeds. The proposed reforms can be split into two categories:'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 11}, page_content='Architectural reforms and Trainable BoF (bag-o f-freebies). Architectural reforms included \\nthe implementation of the E-ELAN (extended e ﬃcient layer aggregation network) [75] in \\nthe YOLO-v7 backbone, taking inspiration from research advancements in network e ﬃ-\\nciency. The design of the E-ELAN was guided by the analysis of factors that impact accu-\\nracy and speed, such as memory access cost , input/output channel ratio, and gradient \\npath.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 11}, page_content='path. \\nThe second architectural reform was presented as compound model scaling, as \\nshown in Figure 11. The aim was to cater for a wider scope of application requirements, \\ni.e., certain applications require accuracy to be prioritized, whilst others may prioritize speed. Although NAS (network architecture se arch) [76] can be used for parameter-spe-\\nciﬁc scaling to ﬁnd the best factors, the scaling factors are independent [77]. Whereas the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 11}, page_content='compound-scaling mechanism a llows for the width and depth to be scaled in coherence \\nfor concatenation-based networks, maintaining optimal network architecture while scal-\\ning for di ﬀerent sizes. \\nFigure 10. Relative evaluation of YOLO-v6 vs. YOLO-v5 [71].\\n2.7. YOLO-v7\\nThe following month after the release of YOLO-v6, the YOLO-v7 was released [ 72].\\nAlthough other variants have been released in between, including YOLO-X [ 73] and YOLO-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 11}, page_content='R [74], these focused more on GPU speed enhancements with respect to inferencing. YOLO-\\nv7 proposes several architectural reforms for improving the accuracy and maintaining high\\ndetection speeds. The proposed reforms can be split into two categories: Architectural\\nreforms and Trainable BoF (bag-of-freebies). Architectural reforms included the implemen-\\ntation of the E-ELAN (extended efﬁcient layer aggregation network) [ 75] in the YOLO-v7'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 11}, page_content='backbone, taking inspiration from research advancements in network efﬁciency. The design\\nof the E-ELAN was guided by the analysis of factors that impact accuracy and speed, such\\nas memory access cost, input/output channel ratio, and gradient path.\\nThe second architectural reform was presented as compound model scaling, as shown\\nin Figure 11. The aim was to cater for a wider scope of application requirements, i.e., certain'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 11}, page_content='applications require accuracy to be prioritized, whilst others may prioritize speed. Although\\nNAS (network architecture search) [ 76] can be used for parameter-speciﬁc scaling to ﬁnd\\nthe best factors, the scaling factors are independent [ 77]. Whereas the compound-scaling\\nmechanism allows for the width and depth to be scaled in coherence for concatenation-\\nbased networks, maintaining optimal network architecture while scaling for different sizes.\\nMachines 2023 , 11, x FOR PEER REVIEW 13 of 26'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 11}, page_content='Figure 11. YOLO-v7 compound scaling. \\nRe-parameterization planning is based on averaging a set of model weights to obtain \\na more robust network [78,79]. Expanding further, module level re-parameterization ena-\\nbles segments of the network to regulate th eir own parameterization strategies. YOLO-v7 \\nutilizes gradient ﬂow propagation paths with the aim to observe which internal network \\nmodules should deploy re-parameterization strategies.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 11}, page_content='The auxiliary head coarse-to- ﬁne concept is proposed on  the premise that the net-\\nwork head is quite far downstream; therefore, the auxiliary head is deployed at the middle \\nlayers to assist in the training proces s. However, this would not train as e ﬃciently as the \\nlead head, due to the former not having access to the complete network. \\nFigure 12 presents a performance comparis on of YOLO-v7 with the preceding YOLO'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 11}, page_content='variants on the MS COCO dataset. It is clear from Figure 12 that all YOLO-v7 variants \\nsurpassed the compared object detectors in a ccuracy and speed in the range of 5–160 FPS. \\nIt is, however, important to note, as mentione d by the authors of YOLO-v7, that none of \\nthe YOLO-v7 variants are designed for CPU-based mobile device deployment. YOLO-v7-tiny/v7/W6 variants are designed for edge GPU, consumer GPU, an d cloud GPU, respec-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 11}, page_content='tively. Whilst YOLO-v7-E6/D6/E6E are designed for high-end cloud GPUs only. \\n \\nFigure 12. YOLO-v7 comparison vs. other object detectors [72]. \\nFigure 11. YOLO-v7 compound scaling.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 12}, page_content='Machines 2023 ,11, 677 13 of 25\\nRe-parameterization planning is based on averaging a set of model weights to obtain a\\nmore robust network [ 78,79]. Expanding further, module level re-parameterization enables\\nsegments of the network to regulate their own parameterization strategies. YOLO-v7\\nutilizes gradient ﬂow propagation paths with the aim to observe which internal network\\nmodules should deploy re-parameterization strategies.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 12}, page_content='The auxiliary head coarse-to-ﬁne concept is proposed on the premise that the network\\nhead is quite far downstream; therefore, the auxiliary head is deployed at the middle layers\\nto assist in the training process. However, this would not train as efﬁciently as the lead\\nhead, due to the former not having access to the complete network.\\nFigure 12 presents a performance comparison of YOLO-v7 with the preceding YOLO'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 12}, page_content='variants on the MS COCO dataset. It is clear from Figure 12 that all YOLO-v7 variants\\nsurpassed the compared object detectors in accuracy and speed in the range of 5–160 FPS .\\nIt is, however, important to note, as mentioned by the authors of YOLO-v7, that none of\\nthe YOLO-v7 variants are designed for CPU-based mobile device deployment. YOLO-\\nv7-tiny/v7/W6 variants are designed for edge GPU, consumer GPU, and cloud GPU,'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 12}, page_content='respectively. Whilst YOLO-v7-E6/D6/E6E are designed for high-end cloud GPUs only.\\nMachines 2023 , 11, x FOR PEER REVIEW 13 of 26 \\n \\n  \\nFigure 11. YOLO-v7 compound scaling. \\nRe-parameterization planning is based on averaging a set of model weights to obtain \\na more robust network [78,79]. Expanding further, module level re-parameterization ena-\\nbles segments of the network to regulate th eir own parameterization strategies. YOLO-v7'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 12}, page_content='utilizes gradient ﬂow propagation paths with the aim to observe which internal network \\nmodules should deploy re-parameterization strategies. \\nThe auxiliary head coarse-to- ﬁne concept is proposed on  the premise that the net-\\nwork head is quite far downstream; therefore, the auxiliary head is deployed at the middle \\nlayers to assist in the training proces s. However, this would not train as e ﬃciently as the \\nlead head, due to the former not having access to the complete network.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 12}, page_content='Figure 12 presents a performance comparis on of YOLO-v7 with the preceding YOLO \\nvariants on the MS COCO dataset. It is clear from Figure 12 that all YOLO-v7 variants \\nsurpassed the compared object detectors in a ccuracy and speed in the range of 5–160 FPS. \\nIt is, however, important to note, as mentione d by the authors of YOLO-v7, that none of'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 12}, page_content='the YOLO-v7 variants are designed for CPU-based mobile device deployment. YOLO-v7-tiny/v7/W6 variants are designed for edge GPU, consumer GPU, an d cloud GPU, respec-\\ntively. Whilst YOLO-v7-E6/D6/E6E are designed for high-end cloud GPUs only. \\n \\nFigure 12. YOLO-v7 comparison vs. other object detectors [72]. \\nFigure 12. YOLO-v7 comparison vs. other object detectors [72].\\nInternal variant comparison of YOLO-v7 is presented in Table 3. As evident, there is a'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 12}, page_content='signiﬁcant performance gap with respect to mAP when comparing YOLO-v7-tiny with the\\ncomputationally demanding YOLO-v7-D6. However, the latter would not be suitable for\\nedge deployment onto a computationally constrained device.\\nTable 3. Variant comparison of YOLO-v7.\\nModel Size (Pixels) mAP (@50) Parameters FLOPs\\nYOLO-v7-tiny 640 52.8% 6.2 M 5.8G\\nYOLO-v7 640 69.7% 36.9 M 104.7G\\nYOLO-v7-X 640 71.1% 71.3 M 189.9G\\nYOLO-v7-E6 1280 73.5% 97.2 M 515.2G\\nYOLO-v7-D6 1280 73.8% 154.7 M 806.8G'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 13}, page_content='Machines 2023 ,11, 677 14 of 25\\n2.8. YOLO-v8\\nThe latest addition to the family of YOLO was conﬁrmed in January 2023 with the\\nrelease of YOLO-v8 [ 80] by Ultralytics (also released YOLO-v5). Although a paper release\\nis impending and many features are yet to be added to the YOLO-v8 repository, initial\\ncomparisons of the newcomer against its predecessors demonstrate its superiority as the\\nnew YOLO state-of-the-art.\\nFigure 13 demonstrates that when comparing YOLO-v8 against YOLO-v5 and YOLO-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 13}, page_content='v6 trained on 640 image resolution, all YOLO-v8 variants output better throughput with a\\nsimilar number of parameters, indicating toward hardware-efﬁcient, architectural reforms.\\nThe fact that YOLO-v8 and YOLO-v5 are presented by Ultralytics with YOLO-v5 providing\\nimpressive real-time performance and based on the initial benchmarking results released\\nby Ultralytics, it is strongly assumed that the YOLO-v8 will be focusing on constrained\\nedge device deployment at high-inference speed.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 13}, page_content='Machines 2023 , 11, x FOR PEER REVIEW 15 of 26 \\n \\n  \\nFigure 13. YOLO-v8 comparison with predecessors [80]. \\n3. Industrial Defect Detection Via YOLO \\nThe previous section demonstrates the rapid evolution of the YOLO ‘clan’ of object \\ndetectors amongst the computer vision community. This section of the review focuses on'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 13}, page_content='the implementation of YOLO variants for the detection of surface defects within the in-dustrial se tting. The selection of ‘industrial se tting’ is due to its varying and stringent re-\\nquirements alternating between accuracy and speed, a theme which is found through \\nDNA of the YOLO variants. \\n3.1. Industrial Fabric Defect Detection \\nRui Jin et al. [81] in their premise state the ine ﬃciencies of manual inspection in the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 13}, page_content='textile manufacturing domain as high cost of  labor, human-related fatigue, and reduced \\ndetection speed (less than 20 m/min). The authors aim to address these ine ﬃciencies by \\nproposing a YOLO-v5-based architecture, coupled with a spatial a ttention mechanism for \\naccentuation of smaller defective regions. Th e proposed approach involved a teacher net-\\nwork trained on the fabric dataset. Post tr aining of the teacher network, the learned'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 13}, page_content='weights were distilled to the student network, which was compatible for deployment onto \\na Jetson TX2 [82] via TensorRT [83]. The results presented by the authors show, as ex-\\npected, that the teacher network reported hi gher performance with an AUC of 98.1% com-\\npared to 95.2% (student network). However, as the student network was computationally \\nsmaller, the inference time was signi ﬁcantly less at 16 ms for the student network in con-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 13}, page_content='trast to the teacher network at 35 ms on th e Jetson TX2. Based on the performance, the \\nFigure 13. YOLO-v8 comparison with predecessors [80].\\n3. Industrial Defect Detection via YOLO\\nThe previous section demonstrates the rapid evolution of the YOLO ‘clan’ of object\\ndetectors amongst the computer vision community. This section of the review focuses\\non the implementation of YOLO variants for the detection of surface defects within the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 13}, page_content='industrial setting. The selection of ‘industrial setting’ is due to its varying and stringent\\nrequirements alternating between accuracy and speed, a theme which is found through\\nDNA of the YOLO variants.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 14}, page_content='Machines 2023 ,11, 677 15 of 25\\n3.1. Industrial Fabric Defect Detection\\nRui Jin et al. [ 81] in their premise state the inefﬁciencies of manual inspection in the\\ntextile manufacturing domain as high cost of labor, human-related fatigue, and reduced\\ndetection speed (less than 20 m/min). The authors aim to address these inefﬁciencies by\\nproposing a YOLO-v5-based architecture, coupled with a spatial attention mechanism'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 14}, page_content='for accentuation of smaller defective regions. The proposed approach involved a teacher\\nnetwork trained on the fabric dataset. Post training of the teacher network, the learned\\nweights were distilled to the student network, which was compatible for deployment onto\\na Jetson TX2 [ 82] via TensorRT [ 83]. The results presented by the authors show, as expected,\\nthat the teacher network reported higher performance with an AUC of 98.1% compared to'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 14}, page_content='95.2% (student network). However, as the student network was computationally smaller,\\nthe inference time was signiﬁcantly less at 16 ms for the student network in contrast to the\\nteacher network at 35 ms on the Jetson TX2. Based on the performance, the authors claim\\nthat the proposed solution provides high accuracy and real-time inference speed, making it\\ncompatible for deployment via the edge device.\\nSifundvoleshile Dlamini et al. [ 84] propose a production environment fabric defect'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 14}, page_content='detection framework focused on real-time detection and accurate classiﬁcation on-site,\\nas shown in Figure 14. The authors embed conventional image processing at the onset\\nof their data enhancement strategy, i.e., ﬁltering to denoise feature enhancement. Post\\naugmentations and data scaling, the authors train the YOLO-v4 architecture based on\\npretrained weights. The reported performance was respectable with an F1-score of 93.6%,'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 14}, page_content='at an impressive detection speed of 34 fps and prediction speed of 21.4 ms. The authors\\nclaim that the performance is evident to the effectiveness of the selected architecture for the\\ngiven domain.\\nMachines 2023 , 11, x FOR PEER REVIEW 16 of 26 \\n \\n authors claim that the proposed solution prov ides high accuracy and real-time inference \\nspeed, making it compatible for deployment via the edge device. \\nSifundvoleshile Dlamini et al. [84] propos e a production environment fabric defect'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 14}, page_content='detection framework focused on real-time detection and accurate classi ﬁcation on-site, as \\nshown in Figure 14. The authors embed conventional image processing at the onset of \\ntheir data enhancement strategy, i.e., ﬁltering to denoise feature enhancement. Post aug-\\nmentations and data scaling, the authors tr ain the YOLO-v4 architecture based on pre-\\ntrained weights. The reported performance was respectable with an F1-score of 93.6%, at'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 14}, page_content='an impressive detection speed of 34 fps and pr ediction speed of 21.4 ms. The authors claim \\nthat the performance is evident to the e ﬀectiveness of the selected architecture for the \\ngiven domain. \\n \\nFigure 14. Inspection machine integration [84]. \\nRestricted by the available computing reso urces for edge deploy ment, Guijuan Lin et \\nal. [85] state problems with quality inspection  in the fabric production domain, including'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 14}, page_content='minute scale of defects, extreme unbalance wi th the aspect ratio of certain defects, and \\nslow defect detection speeds. To address thes e issues, the authors proposed a sliding-win-\\ndow, self-a ttention (multihead) mechanism calibrated  for small defect targets. Addition-\\nally, the Swin Transformer [86] module as depicted in Figure 15 was integrated into the \\noriginal YOLO-v5 architecture for the extraction  of hierarchical features. Furthermore, the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 14}, page_content='generalized focal loss is implemented with the architecture aimed at improving the learn-ing process for positive target instances, whilst lowering the rate of missed detections. The \\nauthors report the accuracy of the proposed so lution on a real-world fabric dataset, reach-\\ning 76.5% mAP at 58.8 FPS, making it compatible with the real-time detection require-ments for detection via embedded devices. \\nFigure 14. Inspection machine integration [84].'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 14}, page_content='Restricted by the available computing resources for edge deployment, Guijuan Lin et al. [ 85]\\nstate problems with quality inspection in the fabric production domain, including minute\\nscale of defects, extreme unbalance with the aspect ratio of certain defects, and slow defect\\ndetection speeds. To address these issues, the authors proposed a sliding-window, self-\\nattention (multihead) mechanism calibrated for small defect targets. Additionally, the Swin'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 14}, page_content='Transformer [ 86] module as depicted in Figure 15 was integrated into the original YOLO-v5\\narchitecture for the extraction of hierarchical features. Furthermore, the generalized focal\\nloss is implemented with the architecture aimed at improving the learning process for\\npositive target instances, whilst lowering the rate of missed detections. The authors report\\nthe accuracy of the proposed solution on a real-world fabric dataset, reaching 76.5% mAP'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 15}, page_content='Machines 2023 ,11, 677 16 of 25\\nat 58.8 FPS, making it compatible with the real-time detection requirements for detection\\nvia embedded devices.\\nMachines 2023 , 11, x FOR PEER REVIEW 17 of 26 \\n \\n  \\nFigure 15. Backbone for Swin Transformer network [85]. \\n3.2. Solar Cell Surface Defect Detection \\nSetting their premise, the authors [87] state that human-led Photovoltaic (PV) inspec-\\ntion has many drawbacks including the re quirement of operation and maintenance'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 15}, page_content='(O&M) engineers, cell-by-cell inspec tion, high workload, and reduced e ﬃciency. The au-\\nthors propose an improved architecture based on YOLO-v5 for the characterization of complex solar cell surface textures and defect ive regions. The proposal is based on the \\nintegration of deformable convolution within  the CSP module with the aim of achieving \\nan adaptive learning scale. Additionally, an a ttention mechanism is incorporated for en-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 15}, page_content='hanced feature extraction. Moreover, the authors optimize the original YOLO-v5 architec-\\nture further via K-means++ clustering for an chor box determination algorithm. Based on \\nthe presented results, the improved architectu re achieved a respectable mAP of 89.64% on \\nan EL-based solar cell image dataset, 7.85% higher compared to mAP for the original ar-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 15}, page_content='chitecture, with detection speed reaching 36.24 FPS, which can be translated as a more accurate detection while remaining compatible with the real-time requirements. \\nAmran Binomairah et al. [88]  highlight two frequent defects encountered during the \\nmanufacturing process of crystalline solar cell s as dark spot/region and microcracks. The \\nlatter can have a detrimental impact on the performance of the module, which is a major'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 15}, page_content='cause for PV module failures. The authors su bscribe to the YOLO architecture, comparing \\nthe performance of their methodology on YOLO-v4 and an improved YOLO-v4-tiny inte-\\ngrated with a spatial pyramid pooling mechanism. Based on the presented results, YOLO-\\nv4 achieved 98.8% mAP at 62.9 ms, whilst the improved YOLO-v4-tiny lagged with 91% \\nmAP at 28.2 ms. The authors claim that although the la tter is less accurate, it is notably \\nfaster than the former.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 15}, page_content='Tianyi Sun et al. [89] focus on automated hot-spot detection within PV cells based a \\nmodi ﬁed version of the YOLO-v5 architecture. The ﬁrst improvement comes in the form \\nof enhanced anchors and detection heads for the respective architecture. To improve the \\ndetection precision at varying scales, k-means clustering [48] is deployed for clustering \\nthe length–width ratio with respect to the data  annotation frame. Additionally, a set of the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 15}, page_content='anchors consisting of smaller values were adde d to cater for the detection of small defects \\nby optimizing the cluster numb er. The reported performance of the improved architecture \\nwas reported as 87.8% mAP, with the average recall rate of 89.0% and F1-score reaching \\n88.9%. The reported FPS was impressive reaching  98.6 FPS, with the authors claiming that \\nthe proposed solution would provide intellige nt monitoring at PV power stations. Infer-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 15}, page_content='encing output presented in Figure 16 sh ows the proposed AP-YOLO-v5 architecture, \\nproviding inferences at a higher con ﬁdence level compared to the original YOLO-v5. \\nFigure 15. Backbone for Swin Transformer network [85].\\n3.2. Solar Cell Surface Defect Detection\\nSetting their premise, the authors [ 87] state that human-led Photovoltaic (PV) inspec-\\ntion has many drawbacks including the requirement of operation and maintenance (O&M)'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 15}, page_content='engineers, cell-by-cell inspection, high workload, and reduced efﬁciency. The authors\\npropose an improved architecture based on YOLO-v5 for the characterization of complex\\nsolar cell surface textures and defective regions. The proposal is based on the integration\\nof deformable convolution within the CSP module with the aim of achieving an adaptive\\nlearning scale. Additionally, an attention mechanism is incorporated for enhanced feature'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 15}, page_content='extraction. Moreover, the authors optimize the original YOLO-v5 architecture further via\\nK-means++ clustering for anchor box determination algorithm. Based on the presented\\nresults, the improved architecture achieved a respectable mAP of 89.64% on an EL-based\\nsolar cell image dataset, 7.85% higher compared to mAP for the original architecture, with\\ndetection speed reaching 36.24 FPS, which can be translated as a more accurate detection\\nwhile remaining compatible with the real-time requirements.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 15}, page_content='Amran Binomairah et al. [ 88] highlight two frequent defects encountered during\\nthe manufacturing process of crystalline solar cells as dark spot/region and microcracks.\\nThe latter can have a detrimental impact on the performance of the module, which is\\na major cause for PV module failures. The authors subscribe to the YOLO architecture,\\ncomparing the performance of their methodology on YOLO-v4 and an improved YOLO-v4-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 15}, page_content='tiny integrated with a spatial pyramid pooling mechanism. Based on the presented results,\\nYOLO-v4 achieved 98.8% mAP at 62.9 ms, whilst the improved YOLO-v4-tiny lagged with\\n91% mAP at 28.2 ms. The authors claim that although the latter is less accurate, it is notably\\nfaster than the former.\\nTianyi Sun et al. [ 89] focus on automated hot-spot detection within PV cells based a\\nmodiﬁed version of the YOLO-v5 architecture. The ﬁrst improvement comes in the form'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 15}, page_content='of enhanced anchors and detection heads for the respective architecture. To improve the\\ndetection precision at varying scales, k-means clustering [ 48] is deployed for clustering the\\nlength–width ratio with respect to the data annotation frame. Additionally, a set of the\\nanchors consisting of smaller values were added to cater for the detection of small defects\\nby optimizing the cluster number. The reported performance of the improved architecture'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 15}, page_content='was reported as 87.8% mAP , with the average recall rate of 89.0% and F1-score reaching'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 16}, page_content='Machines 2023 ,11, 677 17 of 25\\n88.9%. The reported FPS was impressive reaching 98.6 FPS, with the authors claiming\\nthat the proposed solution would provide intelligent monitoring at PV power stations.\\nInferencing output presented in Figure 16 shows the proposed AP-YOLO-v5 architecture,\\nproviding inferences at a higher conﬁdence level compared to the original YOLO-v5.\\nMachines 2023 , 11, x FOR PEER REVIEW 18 of 26 \\n \\n  \\nFigure 16. inference/con ﬁdence comparison [89].'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 16}, page_content='3.3. Steel Surface Defect Detection \\nDinming Yang et al. [90] set the premise of  their research by stating the importance \\nof steel pipe quality inspection, citing the growing demand in countries, such as China. \\nAlthough X-ray testing is utilized as one of the key methods for industrial nondestructive \\ntesting (NDT), the authors state that it still requires human assistance for the determina-\\ntion, classi ﬁcation, and localization of the defect s. The authors propose the implementa-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 16}, page_content='tion of YOLO-v5 for production-based weld st eel defect detection based on X-ray images \\nof the weld pipe. The authors claim that the trained YOLO-v5 reached a mAP of 98.7% \\n(IoU-0.5), whilst meeting the real-time detect ion requirements of steel pipe production \\nwith a single image detection rate of 0.12 s. \\nZhuxi MA et al. [91] address the issu e of large-scale computation and speci ﬁc hard-\\nware requirements for automated defect detection in aluminum strips. The authors select'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 16}, page_content='YOLO-v4 as the architecture, whilst the back bone is constructed to make use of depth-\\nwise separable convolutions along with a parallel dual a ttention mechanism for feature \\nenhancement, as shown in Figure 17. The propos ed network is tested on real data from a \\ncold-rolling workshop, providing impressive results on real data achieving an mAP of \\n96.28%. Compared to the original YOLO-v4, the authors claim that the proposed architec-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 16}, page_content='ture volume is reduced by 83.38%, whilst the inference speed is increased by a factor of \\nthree. The increase in performance was pa rtly due to the custom anchor approach, \\nwhereby due to the maximum aspect ratio of the custom dataset, the defect was set to 1:20 \\nwhich is in-line with the defect characteristics, such as scratch marks. \\nFigure 16. Inference/conﬁdence comparison [89].\\n3.3. Steel Surface Defect Detection'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 16}, page_content='Dinming Yang et al. [ 90] set the premise of their research by stating the importance\\nof steel pipe quality inspection, citing the growing demand in countries, such as China.\\nAlthough X-ray testing is utilized as one of the key methods for industrial nondestructive\\ntesting (NDT), the authors state that it still requires human assistance for the determination,\\nclassiﬁcation, and localization of the defects. The authors propose the implementation of'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 16}, page_content='YOLO-v5 for production-based weld steel defect detection based on X-ray images of the\\nweld pipe. The authors claim that the trained YOLO-v5 reached a mAP of 98.7% (IoU-0.5),\\nwhilst meeting the real-time detection requirements of steel pipe production with a single\\nimage detection rate of 0.12 s.\\nZhuxi MA et al. [ 91] address the issue of large-scale computation and speciﬁc hard-\\nware requirements for automated defect detection in aluminum strips. The authors select'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 16}, page_content='YOLO-v4 as the architecture, whilst the backbone is constructed to make use of depth-\\nwise separable convolutions along with a parallel dual attention mechanism for feature\\nenhancement, as shown in Figure 17. The proposed network is tested on real data from\\na cold-rolling workshop, providing impressive results on real data achieving an mAP of\\n96.28%. Compared to the original YOLO-v4, the authors claim that the proposed architec-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 16}, page_content='ture volume is reduced by 83.38%, whilst the inference speed is increased by a factor of\\nthree. The increase in performance was partly due to the custom anchor approach, whereby\\ndue to the maximum aspect ratio of the custom dataset, the defect was set to 1:20 which is\\nin-line with the defect characteristics, such as scratch marks.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 17}, page_content='Machines 2023 ,11, 677 18 of 25\\nMachines 2023 , 11, x FOR PEER REVIEW 19 of 26 \\n \\n  \\nFigure 17. Proposed parallel network structure [91]. \\nJianting Shi et al. [92] cite the manufacturin g process of steel production as the reason \\nfor various defects originating on the steel surface, such as rolling scale and patches. The \\nauthors state that the small dimensions of the defects as well as the stringent detection'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 17}, page_content='requirements make the quality inspection process a challenging task. Therefore, the au-\\nthors present an improved version of YOLO-v5 by incorporating an a ttention mechanism \\nfor facilitating the transmission of shallow features from the backbone to the neck, pre-\\nserving the defective regions, in addition to k-means clustering of anchor boxes for ad-\\ndressing the extreme aspect ratios of defectiv e targets within the dataset. The authors state'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 17}, page_content='that the improved architecture achieved 86.35% mAP reaching 45 FPS detection speed, whilst the original architecture achieved 81.78% mAP at 52 FPS. \\n3.4. Pallet Racking Defect Inspection \\nA promising application with signi ﬁcant deployment scope in the warehousing and \\ngeneral industrial storage centers is automated pallet racking inspection. Warehouses and \\ndistribution centers host a crit ical infrastructure known as racking for stock storage. Un-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 17}, page_content='noticed damage to pallet racking can pave the way for signi ﬁcant losses initiated by rack-\\ning collapse leading to wasted/damaged stock, ﬁnancial implications, operational losses, \\ninjured employees, and worst-case, loss of lives [93]. Due to the ine ﬃciencies of the con-\\nventional racking inspection mechanisms, such  as human-led annual inspection resulting \\nin labor costs, bias, fatigue, and mechanical pr oducts, such as rackguar ds [94] lacking clas-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 17}, page_content='siﬁcation intelligence, CNN-based automated de tection seems to be a promising alterna-\\ntive. \\nRealizing the potential, Hussain et al. [95] inaugurated research into automated pallet \\nracking detection via computer vision. After presenting their initial research based on the \\nMobileNet-V2 architecture, the authors recent ly proposed the implementation of YOLO-\\nv7 for automated pallet racking inspection [96]. The selection of the architecture was in-'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 17}, page_content='line with the stringent requirements of production ﬂoor deployment, i.e., edge device de-\\nployment, placed onto an oper ating forklift, requiring real-time detection as the forklift \\napproaches the racking. Evaluating the perfor mance of the proposed solution on a real \\ndataset, the authors claimed an impressive pe rformance of 91.1% mAP running at 19 FPS. \\nFigure 17. Proposed parallel network structure [91].'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 17}, page_content='Jianting Shi et al. [ 92] cite the manufacturing process of steel production as the reason\\nfor various defects originating on the steel surface, such as rolling scale and patches. The\\nauthors state that the small dimensions of the defects as well as the stringent detection\\nrequirements make the quality inspection process a challenging task. Therefore, the authors\\npresent an improved version of YOLO-v5 by incorporating an attention mechanism for'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 17}, page_content='facilitating the transmission of shallow features from the backbone to the neck, preserving\\nthe defective regions, in addition to k-means clustering of anchor boxes for addressing\\nthe extreme aspect ratios of defective targets within the dataset. The authors state that the\\nimproved architecture achieved 86.35% mAP reaching 45 FPS detection speed, whilst the\\noriginal architecture achieved 81.78% mAP at 52 FPS.\\n3.4. Pallet Racking Defect Inspection'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 17}, page_content='A promising application with significant deployment scope in the warehousing\\nand general industrial storage centers is automated pallet racking inspection. Ware-\\nhouses and distribution centers host a critical infrastructure known as racking for stock\\nstorage. Unnoticed damage to pallet racking can pave the way for significant losses\\ninitiated by racking collapse leading to wasted/damaged stock, financial implications,'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 17}, page_content='operational losses, injured employees, and worst-case, loss of lives [ 93]. Due to the\\ninefficiencies of the conventional racking inspection mechanisms, such as human-led\\nannual inspection resulting in labor costs, bias, fatigue, and mechanical products, such\\nas rackguards [ 94] lacking classification intelligence, CNN-based automated detection\\nseems to be a promising alternative.\\nRealizing the potential, Hussain et al. [ 95] inaugurated research into automated pallet'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 17}, page_content='racking detection via computer vision. After presenting their initial research based on the\\nMobileNet-V2 architecture, the authors recently proposed the implementation of YOLO-\\nv7 for automated pallet racking inspection [ 96]. The selection of the architecture was\\nin-line with the stringent requirements of production ﬂoor deployment, i.e., edge device\\ndeployment, placed onto an operating forklift, requiring real-time detection as the forklift'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 18}, page_content='Machines 2023 ,11, 677 19 of 25\\napproaches the racking. Evaluating the performance of the proposed solution on a real\\ndataset, the authors claimed an impressive performance of 91.1% mAP running at 19 FPS.\\nTable 4 presents a comparison of the present research in this emerging ﬁeld. Although\\nmask R-CNN presents the highest accuracy, which is a derivative of the segmentation\\nfamily of architectures with signiﬁcant computational load, this makes it an infeasible'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 18}, page_content='option for deployment. Whereas the proposed approach utilizing YOLO-v7 achieved\\nsimilar accuracy compared to MobileNet-V2, whilst requiring signiﬁcantly less training\\ndata along with inferencing at 19 FPS.\\nTable 4. Racking domain research comparison.\\nResearch Architecture Dataset Size Accuracy FPS\\n[95] MobileNet-V2 19,717 92.7% -----\\n[96] YOLO-v7 2095 91.1% 19\\n[97] Mask-RCNN 75 93.45% -----\\n4. Discussion\\nThe YOLO family of object detectors has had a signiﬁcant impact on improving the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 18}, page_content='potential of computer vision applications. Right from the onset, i.e., the release of the\\nYOLO-v1 in 2015, signiﬁcant breakthroughs were introduced. YOLO-v1 became the ﬁrst\\narchitecture combining the two conventionally separate tasks of bounding box prediction\\nand classiﬁcation into one. YOLO-v2 was released in the following year, introducing archi-\\ntectural improvements and iterative improvements, such as batch normalization, higher'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 18}, page_content='resolution, and anchor boxes. In 2018, YOLO-v3 was released, an extension of previous\\nvariants with enhancements including the introduction of objectness scores for bounding\\nbox predictions added connections for the backbone layers and the ability to generate\\npredictions at three different levels of granularity, leading to improved performance on\\nsmaller object targets.\\nAfter a short delay, YOLO-v4 was released in April 2020, becoming the ﬁrst variant of'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 18}, page_content='the YOLO family not to be authored by the original author Joseph Redmon. Enhancements\\nincluded improved feature aggregation, gifting of the ‘bag of freebies’, and the mish\\nactivation. In a matter of months, YOLO-v5 entered the computer vision territory, becoming\\nthe ﬁrst variant to be released without being accompanied by a paper release. YOLO-v5\\nbased on PyTorch, with an active GitHub repo further delineated the implementation'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 18}, page_content='process, make it accessible to a wider audience. Focused on internal architectural reforms,\\nYOLO-v6 authors redesigned the backbone (EfﬁcientRep) and neck (Rep-PAN) modules,\\nwith an inclination toward hardware efﬁciency. Additionally, anchor-free and the concept\\nof decoupled head was introduced, implying additional layers for feature separation from\\nthe ﬁnal head, which is empirically shown to improve the overall performance. The authors'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 18}, page_content='of YOLO-v7 also focused on architectural reforms, considering the amount of memory\\nrequired to keep layers within memory and the distance required for gradients to back-\\npropagate, i.e., shorter gradients, resulting in enhanced learning capacity. For the ultimate\\nlayer aggregation, the authors implemented E-ELAN, which is an extension of the ELAN\\ncomputational block. The advent of 2023 introduced the latest version of the YOLO family,'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 18}, page_content='YOLO-v8, which was released by Ultralytics. With an impending paper release, initial\\ncomparisons of the latest version against predecessors have shown promising performance\\nwith respect to throughput when compared to similar computational parameters.\\n4.1. Reason for Rising Popularity\\nTable 5 presents a summary of the reviewed YOLO variants based on the underlying\\nframework, backbone, average-precision (AP), and key contributions. It can be observed'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 18}, page_content='from Table 3 that as the variants evolved there was a shift from the conservative Darknet\\nframework to a more accessible one, i.e., PyTorch. The AP presented here is based on\\nCOCO-2017 [ 63] with the exception of YOLO-v1/v2, which are based on VOC-2017 [ 39].'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 19}, page_content='Machines 2023 ,11, 677 20 of 25\\nCOCO-2017 [ 63] consists of over 80 objects designed to represent a vast array of regularly\\nseen object. It contains 121,408 images resulting in 883,331 object annotations with median\\nimage ratio of 640 ×480 pixels. It is important to note that the overall accuracy along with\\ninference capacity depends on the deployed design/training strategies, as demonstrated in\\nthe industrial surface detection section.\\nTable 5. Abstract variant comparison.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 19}, page_content='Variant Framework Backbone AP (%) Comments\\nV1 Darknet Darknet-24 63.4 Only detect a maximum of two objects in the same grid.\\nV2 Darknet Darknet-24 63.4Introduced batch norm, k-means clustering for anchor boxes.\\nCapable of detecting > 9000 categories.\\nV3 Darknet Darknet-53 36.2Utilized multi-scale predictions and spatial pyramid pooling\\nleading to larger receptive ﬁeld.\\nV4 Darknet CSPDarknet-53 43.5 Presented bag-of-freebies including the use of CIoU loss.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 19}, page_content='V5 PyTorch Modiﬁed CSPv7 55.8First variant based in PyTorch, making it available to a wider\\naudience. Incorporated the anchor selection processes into\\nthe YOLO-v5 pipeline.\\nV6 PyTorch EfﬁcientRep 52.5Focused on industrial settings, presented an anchor-free\\npipeline. Presented new loss determination mechanisms\\n(VFL, DFL, and SIoU/GIoU).\\nV7 PyTorch RepConvN 56.8Architectural introductions included E-ELAN for faster\\nconvergence along with a bag-of-freebies including'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 19}, page_content='RepConvN and reparameterization-planning.\\nV8 PyTorch YOLO-v8 53.9Anchor-free reducing the number of prediction boxes whilst\\nspeeding up non-maximum suppression. Pending paper for\\nfurther architectural insights.\\nThe AP metric consists of precision-recall (PR) metrics, deﬁning of a positive prediction\\nusing Intersection over Union, and the handling of multiple object categories. AP provides\\na balanced overview of PR based on the area under the PR curve. IoU facilitates the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 19}, page_content='quantiﬁcation of similarity between predicted kpand ground truth kgbounding boxes as\\nexpressed in (8):\\nIoU =area(\\nkp∩kg)\\narea(\\nkp∪kg) (8)\\nThe rise of YOLO can be attributed to two factors. First, the fact that the architectural\\ncomposition of YOLO variants is compatible for one-stage detection and classiﬁcation\\nmakes it computationally lightweight with respect to other detectors. However, we feel\\nthat efﬁcient architectural composition by itself did not drive the popularity of the YOLO'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 19}, page_content='variants, as other single-stage detectors, such as MobileNets, also serve a similar purpose.\\nThe second reason is the accessibility factor, which was introduced as the YOLO\\nvariants progressed, with YOLO-v5 being the turning point. Expanding further on this\\npoint, the ﬁrst two variants were based on the Darknet framework. Although this pro-\\nvided a degree of ﬂexibility, accessibility was limited to a smaller user base due to the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 19}, page_content='required expertise. Ultralytics, introduced YOLO-v5 based on the PyTorch framework,\\nmaking the architecture available for a wider audience and increasing the potential domain\\nof applications.\\nAs evident from Table 6, the migration to a more accessible framework coupled with\\narchitectural reforms for improved real-time performance sky-rocketed. At present, YOLO-\\nv5 has 34.7 k stars, a signiﬁcant lead compared to its predecessors. From implementation,'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 19}, page_content='YOLO-v5 only required the installation of lightweight python libraries. The architectural\\nreforms indicated that the model training time was reduced, which in turn reduced the ex-\\nperimentation cost attributed to the training process, i.e., GPU utilization. For deployment\\nand testing purposes, researchers have several routes, such as individual/batch images,\\nvideo/webcam feeds, in addition to simple weight conversion to ONXX weights for edge\\ndevice deployment.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 20}, page_content='Machines 2023 ,11, 677 21 of 25\\nTable 6. GitHub popularity comparison.\\nYOLO Variant Stars (K)\\nV3 9.3\\nV4 20.2\\nV5 34.7\\nV6 4.6\\nV7 8.4\\nV8 2.9\\n4.2. YOLO and Industrial Defect Detection\\nManifestations of the fourth industrial revolution can be observed at present in an\\nad-hoc manner, spanning across various industries. With respect to the manufacturing\\nindustry, this revolution can be targeted at the quality inspection processes, which are'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 20}, page_content='vital for assuring efﬁciency and retaining client satisfaction. When focusing on surface\\ndefect detection, as alluded to earlier, the inspection requirements can be more stringent\\nas compared to other applications. This is due to many factors, such as the fact that the\\ndefects may be extremely small, requiring external spectral imaging to expose defects prior\\nto classiﬁcation and due to the fact that the operational setting of the production line may'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 20}, page_content='only provide a small-time window within which inference must be carried out.\\nConsidering the stringent requirements outlined above and benchmarking against the\\nprinciples of YOLO family of variants, forms the conclusion that the YOLO variants have the\\npotential to address both real-time, constrained deployment and small-scale defect detec-\\ntion requirements of industrial-based surface defect detection. YOLO variants have proven'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 20}, page_content='real-time compliance in several industrial environments as shown in [81,84,85,90,95] . An\\ninteresting observation arising from the industrial literature reviewed is the ability for users\\nto modify the internal modules of YOLO variants in order to take care of their speciﬁc ap-\\nplication needs without compromising on real-time compliance, for example [81,87,91,92] ,\\nintroducing attention-mechanisms for accentuation of defective regions.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 20}, page_content='An additional factor, found within the later YOLO variants is sub-variants for each\\nbase architecture, i.e., for YOLO-v5 variants including YOLO-v5-S/M/L, this corresponds\\nto different computational loads with respect to the number of parameters. This ﬂexibility\\nenables researchers to consider a more ﬂexible approach with the architecture selection\\ncriteria based on the industrial requirements, i.e., if real-time inference is required with less'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 20}, page_content='emphasis on optimal mAP , a lightweight variant can be selected, such as YOLO-v5-small\\nrather than YOLO-v5-large.\\n5. Conclusions\\nIn conclusion, this work is the ﬁrst of its type focused on documenting and reviewing\\nthe evolution of the most prevalent single-stage object detector within the computer vision\\ndomain. The review presents the key advancements of each variant, followed by imple-\\nmentation of YOLO architectures within various industrial settings focused on surface'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 20}, page_content='automated real-time surface defect detection.\\nFrom the review, it is clear as the YOLO variants have progressed, latter versions in\\nparticular, YOLO-v5 has focused on constrained edge deployment, a key requirement for\\nmany manufacturing applications. Due to the fact that there is no copyright and patent\\nrestrictions, research anchored around the YOLO architecture, i.e., real-time, lightweight,\\naccurate detection, can be conducted by any individual or research organization, which has'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 20}, page_content='also contributed to the prevalence of this variant.\\nWith YOLO-v8 released in January 2023, showing promising performance with respect\\nto throughput and computational load requirements, it is envisioned that 2023 will see\\nmore variants released by previous or new authors focused on improving the deployment\\ncapacity of the architectures with respect to constrained deployment environments.\\nWith research organizations, such as Ultralytics and Meituan Technical Team taking'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 20}, page_content='a keen interest in the development of YOLO architectures with a focus on edge-friendly'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 21}, page_content='Machines 2023 ,11, 677 22 of 25\\ndeployment, we anticipate further technological advancements in the architectural footprint\\nof YOLO. To cater for constrained deployment, these advancements will need to focus on\\nenergy conservation whilst maintaining high inference rates. Furthermore, we envision\\nthe proliferation of YOLO architectures into production facilities to help with quality\\ninspection pipelines as well as providing stimulus for innovative products as demonstrated'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 21}, page_content='by [96] with an automated pallet racking inspection solution. Along with integration\\ninto a diverse set of hardware and IoT devices, YOLO has the potential to tap into new\\ndomains where computer vision can assist in enhancing existing processes whilst requiring\\nlimited resources.\\nFunding: This research received no external funding.\\nData Availability Statement: Not applicable.\\nConﬂicts of Interest: The authors declare no conﬂict of interest.\\nReferences'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 21}, page_content='References\\n1. Zhang, B.; Quan, C.; Ren, F. Study on CNN in the recognition of emotion in audio and images. In Proceedings of the 2016\\nIEEE/ACIS 15th International Conference on Computer and Information Science (ICIS), Okayama, Japan, 26–29 June 2016.\\n[CrossRef]\\n2. Pollen, D.A. Explicit neural representations, recursive neural networks and conscious visual perception. Cereb. Cortex 2003 ,13,\\n807–814. [CrossRef] [PubMed]'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 21}, page_content='3. Using artiﬁcial neural networks to understand the human brain. Res. Featur. 2022 . [CrossRef]\\n4. Improvement of Neural Networks Artiﬁcial Output. Int. J. Sci. Res. (IJSR) 2017 ,6, 352–361. [CrossRef]\\n5. Dodia, S.; Annappa, B.; Mahesh, P .A. Recent advancements in deep learning based lung cancer detection: A systematic review.\\nEng. Appl. Artif. Intell. 2022 ,116, 105490. [CrossRef]'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 21}, page_content='6. Ojo, M.O.; Zahid, A. Deep Learning in Controlled Environment Agriculture: A Review of Recent Advancements, Challenges and\\nProspects. Sensors 2022 ,22, 7965. [CrossRef] [PubMed]\\n7. Jarvis, R.A. A Perspective on Range Finding Techniques for Computer Vision. IEEE Trans. Pattern Anal. Mach. Intell. 1983 ,P AMI-5 ,\\n122–139. [CrossRef]\\n8. Hussain, M.; Bird, J.; Faria, D.R. A Study on CNN Transfer Learning for Image Classiﬁcation. 11 August 2018. Available online:'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 21}, page_content='https://research.aston.ac.uk/en/publications/a-study-on-cnn-transfer-learning-for-image-classiﬁcation (accessed on 1 January\\n2023).\\n9. Yang, R.; Yu, Y. Artiﬁcial Convolutional Neural Network in Object Detection and Semantic Segmentation for Medical Imaging\\nAnalysis. Front. Oncol. 2021 ,11, 638182. [CrossRef]\\n10. Haupt, J.; Nowak, R. Compressive Sampling vs. Conventional Imaging. In Proceedings of the 2006 International Conference on'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 21}, page_content='Image Processing, Las Vegas, NV , USA, 26–29 June 2006; pp. 1269–1272. [CrossRef]\\n11. Gu, J.; Wang, Z.; Kuen, J.; Ma, L.; Shahroudy, A.; Shuai, B.; Liu, T.; Wang, X.; Wang, G.; Cai, J.; et al. Recent advances in\\nconvolutional neural networks. Pattern Recognit. 2018 ,77, 354–377. [CrossRef]\\n12. Perez, H.; Tah, J.H.M.; Mosavi, A. Deep Learning for Detecting Building Defects Using Convolutional Neural Networks. Sensors\\n2019 ,19, 3556. [CrossRef]'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 21}, page_content='13. Hussain, M.; Al-Aqrabi, H.; Hill, R. PV-CrackNet Architecture for Filter Induced Augmentation and Micro-Cracks Detection\\nwithin a Photovoltaic Manufacturing Facility. Energies 2022 ,15, 8667. [CrossRef]\\n14. Hussain, M.; Dhimish, M.; Holmes, V .; Mather, P . Deployment of AI-based RBF network for photovoltaics fault detection\\nprocedure. AIMS Electron. Electr. Eng. 2020 ,4, 1–18. [CrossRef]'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 21}, page_content='15. Hussain, M.; Al-Aqrabi, H.; Munawar, M.; Hill, R.; Parkinson, S. Exudate Regeneration for Automated Exudate Detection in\\nRetinal Fundus Images. IEEE Access 2022 . [CrossRef]\\n16. Hussain, M.; Al-Aqrabi, H.; Hill, R. Statistical Analysis and Development of an Ensemble-Based Machine Learning Model for\\nPhotovoltaic Fault Detection. Energies 2022 ,15, 5492. [CrossRef]\\n17. Singh, S.A.; Desai, K.A. Automated surface defect detection framework using machine vision and convolutional neural networks.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 21}, page_content='J. Intell. Manuf. 2022 ,34, 1995–2011. [CrossRef]\\n18. Weichert, D.; Link, P .; Stoll, A.; Rüping, S.; Ihlenfeldt, S.; Wrobel, S. A review of machine learning for the optimization of\\nproduction processes. Int. J. Adv. Manuf. Technol. 2019 ,104, 1889–1902. [CrossRef]\\n19. Wang, J.; Ma, Y.; Zhang, L.; Gao, R.X.; Wu, D. Deep learning for smart manufacturing: Methods and applications. J. Manuf. Syst.\\n2018 ,48, 144–156. [CrossRef]'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 21}, page_content='20. Weimer, D.; Scholz-Reiter, B.; Shpitalni, M. Design of deep convolutional neural network architectures for automated feature\\nextraction in industrial inspection. CIRP Ann. 2016 ,65, 417–420. [CrossRef]\\n21. Kusiak, A. Smart manufacturing. Int. J. Prod. Res. 2017 ,56, 508–517. [CrossRef]'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 22}, page_content='Machines 2023 ,11, 677 23 of 25\\n22. Yang, J.; Li, S.; Wang, Z.; Dong, H.; Wang, J.; Tang, S. Using Deep Learning to Detect Defects in Manufacturing: A Comprehensive\\nSurvey and Current Challenges. Materials 2020 ,13, 5755. [CrossRef]\\n23. Soviany, P .; Ionescu, R.T. Optimizing the Trade-Off between Single-Stage and Two-Stage Deep Object Detectors using Image\\nDifﬁculty Prediction. In Proceedings of the 2018 20th International Symposium on Symbolic and Numeric Algorithms for'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 22}, page_content='Scientiﬁc Computing (SYNASC), Timisoara, Romania, 20–23 September 2018. [CrossRef]\\n24. Du, L.; Zhang, R.; Wang, X. Overview of two-stage object detection algorithms. J. Phys. Conf. Ser. 2020 ,1544 , 012033. [CrossRef]\\n25. Sultana, F.; Suﬁan, A.; Dutta, P . A Review of Object Detection Models Based on Convolutional Neural Network. In Advances in\\nIntelligent Systems and Computing ; Springer: Singapore, 2020; pp. 1–16. [CrossRef]'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 22}, page_content='26. Liu, W.; Anguelov, D.; Erhan, D.; Szegedy, C.; Reed, S.; Fu, C.Y.; Berg, A.C. SSD: Single shot multibox detector. In Proceedings of\\nthe Computer Vision—ECCV 2016, Amsterdam, The Netherlands, 11–14 October 2016; pp. 21–37. [CrossRef]\\n27. Fu, C.Y.; Liu, W.; Ranga, A.; Tyagi, A.; Berg, A.C. DSSD: Deconvolutional Single Shot Detector. arXiv 2017 , arXiv:1701.06659.\\n28. Cheng, X.; Yu, J. RetinaNet with Difference Channel Attention and Adaptively Spatial Feature Fusion for Steel Surface Defect'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 22}, page_content='Detection. IEEE Trans. Instrum. Meas. 2020 ,70, 2503911. [CrossRef]\\n29. Redmon, J.; Divvala, S.; Girshick, R.; Farhadi, A. You Only Look Once: Uniﬁed, Real-Time Object Detection. In Proceedings of the\\n2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV , USA, 27–30 June 2016; pp. 779–788.\\n[CrossRef]\\n30. Wang, Z.J.; Turko, R.; Shaikh, O.; Park, H.; Das, N.; Hohman, F.; Kahng, M.; Chau, D.H.P . CNN Explainer: Learning Convolutional'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 22}, page_content='Neural Networks with Interactive Visualization. IEEE Trans. Vis. Comput. Graph. 2020 ,27, 1396–1406. [CrossRef] [PubMed]\\n31. Krizhevsky, A.; Sutskever, I.; Hinton, G.E. Imagenet classiﬁcation with deep convolutional neural networks. Commun. ACM 2017 ,\\n60, 84–90. [CrossRef]\\n32. Simonyan, K.; Zisserman, A. Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv 2014 , arXiv:1409.1556.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 22}, page_content='33. Szegedy, C.; Liu, W.; Jia, Y.; Sermanet, P .; Reed, S.; Anguelov, D.; Rabinovich, A. Going deeper with convolutions. In Proceedings\\nof the Conference on Computer Vision and Pattern Recognition, Boston, MA, USA, 12 June 2015.\\n34. He, K.; Zhang, X.; Ren, S.; Sun, J. Deep residual learning for image recognition. In Proceedings of the Conference on Computer\\nVision and Pattern Recognition, Las Vegas, NV , USA, 30 June 2016.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 22}, page_content='35. Girshick, R.; Donahue, J.; Darrell, T.; Malik, J. Region-Based Convolutional Networks for Accurate Object Detection and\\nSegmentation. IEEE Trans. Pattern Anal. Mach. Intell. 2015 ,38, 142–158. [CrossRef]\\n36. Girshick, R. Fast R-CNN. In Proceedings of the International Conference on Computer Vision, Santiago, Chile, 7–13 December\\n2015.\\n37. Ren, S.; He, K.; Girshick, R.; Sun, J. Faster R-CNN: Towards real-time object detection with region proposal networks. Trans.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 22}, page_content='Pattern Anal. Mach. Intell. 2017 ,39, 1137–1149. [CrossRef]\\n38. Vidyavani, A.; Dheeraj, K.; Reddy, M.R.M.; Kumar, K.N. Object Detection Method Based on YOLOv3 using Deep Learning\\nNetworks. Int. J. Innov. Technol. Explor. Eng. 2019 ,9, 1414–1417. [CrossRef]\\n39. Everingham, M.; Van Gool, L.; Williams, C.K.I.; Winn, J.; Zisserman, A. The Pascal Visual Object Classes (VOC) Challenge. Int. J.\\nComput. Vis. 2009 ,88, 303–338. [CrossRef]'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 22}, page_content='40. Shetty, S. Application of Convolutional Neural Network for Image Classiﬁcation on Pascal VOC Challenge 2012 dataset.\\narXiv 2016 , arXiv:1607.03785.\\n41. Felzenszwalb, P .F.; Girshick, R.B.; McAllester, D.; Ramanan, D. Object Detection with Discriminatively Trained Part-Based Models.\\nIEEE Trans. Pattern Anal. Mach. Intell. 2009 ,32, 1627–1645. [CrossRef] [PubMed]\\n42. Chang, Y.-L.; Anagaw, A.; Chang, L.; Wang, Y.C.; Hsiao, C.-Y.; Lee, W.-H. Ship Detection Based on YOLOv2 for SAR Imagery.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 22}, page_content='Remote Sens. 2019 ,11, 786. [CrossRef]\\n43. Liao, Z.; Carneiro, G. On the importance of normalisation layers in deep learning with piecewise linear activation units. In\\nProceedings of the 2016 IEEE Winter Conference on Applications of Computer Vision (WACV), New York, NY, USA, 7–10 March\\n2016. [CrossRef]\\n44. Garbin, C.; Zhu, X.; Marques, O. Dropout vs. batch normalization: An empirical study of their impact to deep learning. Multimed.\\nTools Appl. 2020 ,79, 12777–12815. [CrossRef]'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 22}, page_content='45. Li, G.; Jian, X.; Wen, Z.; AlSultan, J. Algorithm of overﬁtting avoidance in CNN based on maximum pooled and weight decay.\\nAppl. Math. Nonlinear Sci. 2022 ,7, 965–974. [CrossRef]\\n46. Deng, J.; Dong, W.; Socher, R.; Li, L.J.; Li, K.; Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In Proceedings of the\\n2009 IEEE Conference on Computer Vision and Pattern Recognition, Miami, FL, USA, 20–25 June 2009.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 22}, page_content='47. Xue, J.; Cheng, F.; Li, Y.; Song, Y.; Mao, T. Detection of Farmland Obstacles Based on an Improved YOLOv5s Algorithm by Using\\nCIoU and Anchor Box Scale Clustering. Sensors 2022 ,22, 1790. [CrossRef]\\n48. Ahmed, M.; Seraj, R.; Islam, S.M.S. The k-means Algorithm: A Comprehensive Survey and Performance Evaluation. Electronics\\n2020 ,9, 1295. [CrossRef]\\n49. Redmon, J. Darknet: Open Source Neural Networks in C. 2013. Available online: https://pjreddie.com/darknet (accessed on\\n1 January 2023).'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 22}, page_content='1 January 2023).\\n50. Furusho, Y.; Ikeda, K. Theoretical analysis of skip connections and batch normalization from generalization and optimization\\nperspectives. APSIP A Trans. Signal Inf. Process. 2020 ,9, e9. [CrossRef]'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 23}, page_content='Machines 2023 ,11, 677 24 of 25\\n51. Machine-Learning System Tackles Speech and Object Recognition. Available online: https://news.mit.edu/machine-learning-\\nimage-object-recognition-918 (accessed on 1 January 2023).\\n52. Bochkovskiy, A.; Wang, C.Y.; Liao HY, M. YOLOv4: Optimal Speed and Accuracy of Object Detection. arXiv 2020 ,\\narXiv:2004.10934.\\n53. Tan, M.; Le, Q. EfﬁcientNet: Rethinking model scaling for convolutional neural networks. In Proceedings of the International'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 23}, page_content='Conference on Machine Learning (ICML), Long Beach, CA, USA, 9–15 June 2019.\\n54. Huang, G.; Liu, Z.; Van Der Maaten, L.; Weinberger, K.Q. Densely connected convolutional networks. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 21–26 July 2017; pp. 4700–4708.\\n55. Lin, T.Y.; Doll ár, P .; Girshick, R.; He, K.; Hariharan, B.; Belongie, S. Feature pyramid networks for object detection. In Proceedings'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 23}, page_content='of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 21–26 July 2017; pp. 2117–2125.\\n56. Liu, S.; Qi, L.; Qin, H.; Shi, J.; Jia, J. Path aggregation network for instance segmentation. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition (CVPR), Salt Lake City, UT, USA, 18–23 June 2018; pp. 8759–8768.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 23}, page_content='57. He, K.; Zhang, X.; Ren, S.; Sun, J. Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition. IEEE Trans.\\nPattern Anal. Mach. Intell. 2015 ,37, 1904–1916. [CrossRef]\\n58. Zheng, Z.; Wang, P .; Liu, W.; Li, J.; Ye, R.; Ren, D. Distance-IoU Loss: Faster and better learning for bounding box regression. In\\nProceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI), New York, NY, USA, 7–12 February 2020.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 23}, page_content='59. Misra, D. Mish: A self regularized nonmonotonic neural activation function. arXiv 2019 , arXiv:1908.08681.\\n60. Yao, Z.; Cao, Y.; Zheng, S.; Huang, G.; Lin, S. Cross-Iteration Batch Normalization. arXiv 2020 , arXiv:2002.05712.\\n61. Ultralytics. YOLOv5 2020. Available online: https://github.com/ultralytics/yolov5 (accessed on 1 January 2023).\\n62. Jocher, G.; Stoken, A.; Borovec, J.; Christopher, S.T.A.N.; Laughing, L.C. Ultralytics/yolov5: v4.0-nn.SiLU() Activations, Weights'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 23}, page_content='& Biases Logging, PyTorch Hub Integration. Zenodo 2021 . Available online: https://zenodo.org/record/4418161 (accessed on\\n5 January 2023).\\n63. Lin, T.Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P .; Ramanan, D.; Zitnick, C.L. Microsoft coco: Common objects in context. In\\nProceedings of the European Conference on Computer Vision, Zurich, Switzerland, 6–12 September 2014.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 23}, page_content='64. Tan, M.; Pang, R.; Le, Q.V . EfﬁcientDet: Scalable and Efﬁcient Object Detection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition, Seattle, WA, USA, 13–19 June 2020.\\n65. Li, C.; Li, L.; Jiang, H.; Weng, K.; Geng, Y.; Li, L.; Wei, X. YOLOv6: A Single-Stage Object Detection Framework for Industrial\\nApplications. arXiv 2022 , arXiv:2209.02976.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 23}, page_content='66. Ding, X.; Zhang, X.; Ma, N.; Han, J.; Ding, G.; Sun, J. Repvgg: Making vgg-style convnets great again. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 20–25 June 2021; pp. 13733–13742.\\n67. Zhang, H.; Wang, Y.; Dayoub, F.; Sunderhauf, N. Varifocalnet: An iou-aware dense object detector. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 20–25 June 2021; pp. 8514–8523.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 23}, page_content='68. Li, X.; Wang, W.; Wu, L.; Chen, S.; Hu, X.; Li, J.; Yang, J. Generalized focal loss: Learning qualiﬁed and distributed bounding\\nboxes for dense object detection. Adv. Neural Inf. Process. Syst. 2020 ,33, 21002–21012.\\n69. Gevorgyan, Z. Siou loss: More powerful learning for bounding box regression. arXiv 2022 , arXiv:2205.12740.\\n70. Shu, C.; Liu, Y.; Gao, J.; Yan, Z.; Shen, C. Channel-wise knowledge distillation for dense prediction. In Proceedings of the'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 23}, page_content='IEEE/CVF International Conference on Computer Vision, Montreal, BC, Canada, 11–17 October 20221; pp. 5311–5320.\\n71. Solawetz, J.; Nelson, J. What’s New in YOLOv6? 4 July 2022. Available online: https://blog.roboﬂow.com/yolov6/ (accessed on\\n1 January 2023).\\n72. Wang, C.Y.; Bochkovskiy, A.; Liao HY, M. YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors.\\narXiv 2022 , arXiv:2207.02696.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 23}, page_content='73. Ge, Z.; Liu, S.; Wang, F.; Li, Z.; Sun, J. YOLOX: Exceeding YOLO series in 2021. arXiv 2021 , arXiv:2107.08430.\\n74. Wang, C.-Y.; Yeh, I.-H.; Liao, H.-Y.M. You only learn one representation: Uniﬁed network for multiple tasks. arXiv 2021 ,\\narXiv:2105.04206.\\n75. Wu, W.; Zhao, Y.; Xu, Y.; Tan, X.; He, D.; Zou, Z.; Ye, J.; Li, Y.; Yao, M.; Dong, Z.; et al. DSANet: Dynamic Segment AggrDSANet:'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 23}, page_content='Dynamic Segment Aggregation Network for Video-Level Representation Learning. In Proceedings of the MM ’21—29th ACM\\nInternational Conference on Multimedia, Virtual, 20–24 October 2021. [CrossRef]\\n76. Li, C.; Tang, T.; Wang, G.; Peng, J.; Wang, B.; Liang, X.; Chang, X. BossNAS: Exploring Hybrid CNN-transformers with Block-\\nwisely Self-supervised Neural Architecture Search. In Proceedings of the IEEE/CVF International Conference on Computer\\nVision, Online, 11–17 October 2021. [CrossRef]'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 23}, page_content='77. Dollar, P .; Singh, M.; Girshick, R. Fast and accurate model scaling. In Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR), Nashville, TN, USA, 20–25 June 2021; pp. 924–932.\\n78. Guo, S.; Alvarez, J.M.; Salzmann, M. ExpandNets: Linear over-parameterization to train compact convolutional networks. Adv.\\nNeural Inf. Process. Syst. (NeurIPS) 2020 ,33, 1298–1310.'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 23}, page_content='79. Ding, X.; Zhang, X.; Zhou, Y.; Han, J.; Ding, G.; Sun, J. Scaling up your kernels to 31 ×31: Revisiting large kernel design in CNNs.\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA, 18–24\\nJune 2022.\\n80. Jocher, G.; Chaurasia, A.; Qiu, J. YOLO by Ultralytics. GitHub. 1 January 2023. Available online: https://github.com/ultralytics/\\nultralytics (accessed on 12 January 2023).'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 24}, page_content='Machines 2023 ,11, 677 25 of 25\\n81. Jin, R.; Niu, Q. Automatic Fabric Defect Detection Based on an Improved YOLOv5. Math. Probl. Eng. 2021 ,2021 , 1–13. [CrossRef]\\n82. NVIDIA Jetson TX2: High Performance AI at the Edge, NVIDIA. Available online: https://www.nvidia.com/en-gb/autonomous-\\nmachines/embedded-systems/jetson-tx2/ (accessed on 30 January 2023).\\n83. NVIDIA TensorRT. NVIDIA Developer. 18 July 2019. Available online: https://developer.nvidia.com/tensorrt (accessed on\\n5 January 2023).'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 24}, page_content='5 January 2023).\\n84. Dlamini, S.; Kao, C.-Y.; Su, S.-L.; Kuo, C.-F.J. Development of a real-time machine vision system for functional textile fabric defect\\ndetection using a deep YOLOv4 model. Text. Res. J. 2021 ,92, 675–690. [CrossRef]\\n85. Lin, G.; Liu, K.; Xia, X.; Yan, R. An Efﬁcient and Intelligent Detection Method for Fabric Defects based on Improved YOLOv5.\\nSensors 2022 ,23, 97. [CrossRef] [PubMed]'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 24}, page_content='86. Liu, Z.; Tan, Y.; He, Q.; Xiao, Y. SwinNet: Swin Transformer Drives Edge-Aware RGB-D and RGB-T Salient Object Detection. IEEE\\nTrans. Circuits Syst. Video Technol. 2021 ,32, 4486–4497. [CrossRef]\\n87. Zhang, M.; Yin, L. Solar Cell Surface Defect Detection Based on Improved YOLO v5. IEEE Access 2022 ,10, 80804–80815. [CrossRef]\\n88. Binomairah, A.; Abdullah, A.; Khoo, B.E.; Mahdavipour, Z.; Teo, T.W.; Noor, N.S.M.; Abdullah, M.Z. Detection of microcracks'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 24}, page_content='and dark spots in monocrystalline PERC cells using photoluminescene imaging and YOLO-based CNN with spatial pyramid\\npooling. EPJ Photovolt. 2022 ,13, 27. [CrossRef]\\n89. Sun, T.; Xing, H.; Cao, S.; Zhang, Y.; Fan, S.; Liu, P . A novel detection method for hot spots of photovoltaic (PV) panels using\\nimproved anchors and prediction heads of YOLOv5 network. Energy Rep. 2022 ,8, 1219–1229. [CrossRef]'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 24}, page_content='90. Yang, D.; Cui, Y.; Yu, Z.; Yuan, H. Deep Learning Based Steel Pipe Weld Defect Detection. Appl. Artif. Intell. 2021 ,35, 1237–1249.\\n[CrossRef]\\n91. Ma, Z.; Li, Y.; Huang, M.; Huang, Q.; Cheng, J.; Tang, S. A lightweight detector based on attention mechanism for aluminum strip\\nsurface defect detection. Comput. Ind. 2021 ,136, 103585. [CrossRef]\\n92. Shi, J.; Yang, J.; Zhang, Y. Research on Steel Surface Defect Detection Based on YOLOv5 with Attention Mechanism. Electronics'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 24}, page_content='2022 ,11, 3735. [CrossRef]\\n93. CEP , F.A. 5 Insightful Statistics Related to Warehouse Safety. Available online: www.damotech.com (accessed on 11 January 2023).\\n94. Armour, R. The Rack Group. Available online: https://therackgroup.com/product/rack-armour/ (accessed on 12 January 2023).\\n95. Hussain, M.; Chen, T.; Hill, R. Moving toward Smart Manufacturing with an Autonomous Pallet Racking Inspection System\\nBased on MobileNetV2. J. Manuf. Mater. Process. 2022 ,6, 75. [CrossRef]'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 24}, page_content='96. Hussain, M.; Al-Aqrabi, H.; Munawar, M.; Hill, R.; Alsboui, T. Domain Feature Mapping with YOLOv7 for Automated Edge-Based\\nPallet Racking Inspections. Sensors 2022 ,22, 6927. [CrossRef] [PubMed]\\n97. Farahnakian, F.; Koivunen, L.; Makila, T.; Heikkonen, J. Towards Autonomous Industrial Warehouse Inspection. In Proceedings of\\nthe 2021 26th International Conference on Automation and Computing (ICAC), Portsmouth, UK, 2–4 September 2021. [CrossRef]'),\n",
       " Document(metadata={'source': 'pdf\\\\yolo.pdf', 'page': 24}, page_content='Disclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual\\nauthor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\\npeople or property resulting from any ideas, methods, instructions or products referred to in the content.')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citation: Hussain, M. YOLO-v1 to\n",
      "YOLO-v8, the Rise of YOLO and Its\n",
      "Complementary Nature toward\n",
      "Digital Manufacturing and Industrial\n",
      "Defect Detection. Machines 2023 ,11,\n",
      "677. https://doi.org/10.3390/\n",
      "machines11070677\n",
      "Academic Editor: Sang Do Noh\n",
      "Received: 30 May 2023\n",
      "Revised: 15 June 2023\n",
      "Accepted: 21 June 2023\n",
      "Published: 23 June 2023\n",
      "Copyright: © 2023 by the author.\n",
      "Licensee MDPI, Basel, Switzerland.\n",
      "This article is an open access article\n",
      "distributed under the terms and\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conditions of the Creative Commons\n",
      "Attribution (CC BY) license (https://\n",
      "creativecommons.org/licenses/by/\n",
      "4.0/).\n",
      "machines\n",
      "Review\n",
      "YOLO-v1 to YOLO-v8, the Rise of YOLO and Its\n",
      "Complementary Nature toward Digital Manufacturing and\n",
      "Industrial Defect Detection\n",
      "Muhammad Hussain\n",
      "Department of Computer Science, School of Computing and Engineering, University of Huddersﬁeld,\n",
      "Queensgate, Huddersﬁeld HD1 3DH, UK; m.hussain@hud.ac.uk\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract: Since its inception in 2015, the YOLO (You Only Look Once) variant of object detectors has\n",
      "rapidly grown, with the latest release of YOLO-v8 in January 2023. YOLO variants are underpinned\n",
      "by the principle of real-time and high-classiﬁcation performance, based on limited but efﬁcient\n",
      "computational parameters. This principle has been found within the DNA of all YOLO variants\n",
      "with increasing intensity, as the variants evolve addressing the requirements of automated quality\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= text_chunks\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022 ,11, 3735. [CrossRef]\n",
      "93. CEP , F.A. 5 Insightful Statistics Related to Warehouse Safety. Available online: www.damotech.com (accessed on 11 January 2023).\n",
      "94. Armour, R. The Rack Group. Available online: https://therackgroup.com/product/rack-armour/ (accessed on 12 January 2023).\n",
      "95. Hussain, M.; Chen, T.; Hill, R. Moving toward Smart Manufacturing with an Autonomous Pallet Racking Inspection System\n",
      "Based on MobileNetV2. J. Manuf. Mater. Process. 2022 ,6, 75. [CrossRef]\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[330].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_11048\\2476708891.py:1: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  embedding= OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "embedding= OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb= embedding.embed_query(\"How are you\")\n",
    "len(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTING pINE cONE api kEYS AND ENVIRONMENT\n",
    "\n",
    "PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY','8f81c381-c45e-499d-a898-14d0596697e5')\n",
    "PINECONE_API_ENV=os.environ.get('PINECONE_API_ENV', 'gcp-starter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=1536, # Replace with your model dimensions\n",
    "    metric=\"cosine\", # Replace with your model metric\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index = pinecone.Index(\"testing\", host=\"https://testing-5urv56c.svc.aped-4627-b74a.pinecone.io\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Embeddings - upserts for text chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Set the API key directly in the code (for testing purposes only)\n",
    "os.environ['PINECONE_API_KEY'] = '8f81c381-c45e-499d-a898-14d0596697e5'\n",
    "\n",
    "# Now retrieve the API key\n",
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
    "\n",
    "if not PINECONE_API_KEY:\n",
    "    raise ValueError(\"PINECONE_API_KEY is not set in the environment variables\")\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone as LangchainPinecone\n",
    "\n",
    "# ... (after creating the index)\n",
    "\n",
    "docsearch = LangchainPinecone.from_texts([t.page_content for t in text_chunks], embedding, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.pinecone.Pinecone at 0x1f8ad398250>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity Search\n",
    "query = \"which model does YOLOv7 outperform and by how much\"\n",
    "docs= docsearch.similarity_search(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Figure 12 presents a performance comparis on of YOLO-v7 with the preceding YOLO \\nvariants on the MS COCO dataset. It is clear from Figure 12 that all YOLO-v7 variants \\nsurpassed the compared object detectors in a ccuracy and speed in the range of 5–160 FPS. \\nIt is, however, important to note, as mentione d by the authors of YOLO-v7, that none of'),\n",
       " Document(page_content='model. Comparing the performance of YOLO-v6 with its predecessors, including YOLO-v5\\non the benchmark COCO dataset in Figure 10, it is clear that YOLO-v6 achieves a higher\\nmAP at various FPS.'),\n",
       " Document(page_content='variants on the MS COCO dataset. It is clear from Figure 12 that all YOLO-v7 variants\\nsurpassed the compared object detectors in accuracy and speed in the range of 5–160 FPS .\\nIt is, however, important to note, as mentioned by the authors of YOLO-v7, that none of\\nthe YOLO-v7 variants are designed for CPU-based mobile device deployment. YOLO-\\nv7-tiny/v7/W6 variants are designed for edge GPU, consumer GPU, and cloud GPU,'),\n",
       " Document(page_content='signiﬁcant performance gap with respect to mAP when comparing YOLO-v7-tiny with the\\ncomputationally demanding YOLO-v7-D6. However, the latter would not be suitable for\\nedge deployment onto a computationally constrained device.\\nTable 3. Variant comparison of YOLO-v7.\\nModel Size (Pixels) mAP (@50) Parameters FLOPs\\nYOLO-v7-tiny 640 52.8% 6.2 M 5.8G\\nYOLO-v7 640 69.7% 36.9 M 104.7G\\nYOLO-v7-X 640 71.1% 71.3 M 189.9G\\nYOLO-v7-E6 1280 73.5% 97.2 M 515.2G\\nYOLO-v7-D6 1280 73.8% 154.7 M 806.8G')]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrival from LLM\n",
    "#qa=RetrievalQA.from_chain_type(llm=llm,chain_type=\"stuff\",retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "retriever = docsearch.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"which models does YOLOv7 Outperform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "while True:\n",
    "    user_input = input(f\"Input Prompt:\")\n",
    "    if user_input == 'exit':\n",
    "        print(\"Exiting\")\n",
    "        sys.exit()\n",
    "    if user_input == \"\":\n",
    "        continue\n",
    "    result= qa({'query': user_input})\n",
    "    print(f\"Answer:{result['result']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
